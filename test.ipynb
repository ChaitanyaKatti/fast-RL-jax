{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff735c24",
   "metadata": {},
   "source": [
    "# PPO Clip Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8743618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters (example values similar to PPO paper)\n",
    "mu_old = 0.0           # old policy mean\n",
    "log_std_old = 0.0      # old policy log std (σ_old = 1)\n",
    "sigma_old = np.exp(log_std_old)\n",
    "\n",
    "a_t = 0.0              # action taken\n",
    "eps = 0.2              # PPO clip epsilon\n",
    "\n",
    "# Grid for μ_t and log σ_t\n",
    "mu_range = np.linspace(-1.2, 1.2, 300)\n",
    "log_std_range = np.linspace(-1.4, 0.5, 300)\n",
    "MU, LOG_STD = np.meshgrid(mu_range, log_std_range)\n",
    "SIGMA = np.exp(LOG_STD)\n",
    "\n",
    "# Compute log r_t\n",
    "log_r = (log_std_old - LOG_STD) + \\\n",
    "        ((a_t - mu_old)**2) / (2 * sigma_old**2) - \\\n",
    "        ((a_t - MU)**2) / (2 * SIGMA**2)\n",
    "\n",
    "# Clip bounds in log space\n",
    "lower_bound = np.log(1 - eps)\n",
    "upper_bound = np.log(1 + eps)\n",
    "\n",
    "# Region mask\n",
    "mask = (log_r >= lower_bound) & (log_r <= upper_bound)\n",
    "\n",
    "# Compute KL divergence D_KL( pi_old || pi_new ) for Gaussian policies\n",
    "KL = np.log(SIGMA / sigma_old) + \\\n",
    "     (sigma_old**2 + (MU - mu_old)**2) / (2 * SIGMA**2) - 0.5\n",
    "\n",
    "# Plot with KL contours\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.contour(MU, LOG_STD, log_r, levels=[lower_bound, upper_bound],\n",
    "            colors='k', linestyles='--', linewidths=1.5, alpha=0.8)\n",
    "plt.imshow(1-mask, extent=[mu_range.min(), mu_range.max(), log_std_range.min(), log_std_range.max()],\n",
    "           origin='lower', alpha=0.15, cmap='gray')\n",
    "\n",
    "# KL divergence contours\n",
    "kl_levels = [0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0]\n",
    "contours = plt.contour(MU, LOG_STD, KL, levels=kl_levels, cmap='coolwarm')\n",
    "plt.clabel(contours, inline=True, fontsize=8, fmt=\"%.2f\")\n",
    "\n",
    "# Mark theta_old\n",
    "plt.scatter(mu_old, log_std_old, color='red', label=r'$\\theta_{\\mathrm{old}}$')\n",
    "plt.xlabel(r'$\\mu_t$')\n",
    "plt.ylabel(r'$\\log \\Sigma_t$')\n",
    "plt.legend()\n",
    "plt.title(\"PPO Clipping Region with KL Divergence Contours\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5db7a6",
   "metadata": {},
   "source": [
    "# Max Tanh Gaussian Distribution Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65047811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1284e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_range = np.linspace(-3, 3, 300)\n",
    "log_std_range = np.linspace(-4, 2, 300)\n",
    "MU, LOG_STD = np.meshgrid(mu_range, log_std_range)\n",
    "SIGMA = np.exp(LOG_STD)\n",
    "\n",
    "def gaussian_pdf(x, mu, log_std):\n",
    "    sigma = np.exp(log_std)\n",
    "    z = (x - mu) / sigma\n",
    "    return 0.5 * np.exp(-0.5 * z**2) / (sigma * np.sqrt(2 * np.pi))\n",
    "\n",
    "def tanh_gaussian_pdf(x, mu, log_std):\n",
    "    return gaussian_pdf(np.arctanh(x), mu, log_std) / (1 - x**2)\n",
    "\n",
    "def entropy_tanh_gaussian(mu, log_std):\n",
    "    N, M = mu.shape\n",
    "    assert N == M, \"MU and LOG_STD must be square matrices of the same shape\"\n",
    "    \n",
    "    K = 1000  # Number of points for numerical integration\n",
    "    x = np.linspace(-1.0 + 1e-6, 1.0 - 1e-6, K)\n",
    "    \n",
    "    MU_exp = mu[..., np.newaxis]            # (N, N, 1)\n",
    "    LOG_STD_exp = log_std[..., np.newaxis]  # (N, N, 1)\n",
    "    x_exp = x[np.newaxis, np.newaxis, :]    # (1, 1, K)\n",
    "\n",
    "    MU_broadcast = np.broadcast_to(MU_exp, (N, N, K))\n",
    "    LOG_STD_broadcast = np.broadcast_to(LOG_STD_exp, (N, N, K))\n",
    "    x_broadcast = np.broadcast_to(x_exp, (N, N, K))\n",
    "    \n",
    "    pdf_values = tanh_gaussian_pdf(x_broadcast, MU_broadcast, LOG_STD_broadcast)\n",
    "\n",
    "    # Entropy is calculated as -∫ p(x) log(p(x)) dx\n",
    "    entropy = -np.sum(pdf_values * np.log(pdf_values + 1e-10), axis=-1) * (x[1] - x[0])\n",
    "    return entropy\n",
    "\n",
    "entropy_values = entropy_tanh_gaussian(MU, LOG_STD)\n",
    "log_sigma_for_max_entropy = log_std_range[np.argmax(entropy_values, axis=0)]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(MU, LOG_STD, entropy_values, levels=100, cmap='viridis', alpha=0.8)\n",
    "plt.plot(MU[0, :], log_sigma_for_max_entropy, color='blue', label='Max Entropy Curve')\n",
    "plt.colorbar(label='Entropy')\n",
    "plt.xlabel(r'$\\mu_t$')\n",
    "plt.ylabel(r'$\\log \\Sigma_t$')\n",
    "plt.title('Entropy of Tanh Gaussian Distribution')\n",
    "plt.scatter(0.0, 0.0, color='red', label=r'$\\theta_{\\mathrm{old}}$', s=50)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04cc23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_values.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abc24f2",
   "metadata": {},
   "source": [
    "# Opimal Parameters Fast Entorpy\n",
    "\n",
    "- k-dimensional\n",
    "- N parameters\n",
    "- Optimized for gradients of entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e33b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4bdcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize samples\n",
    "key = jax.random.PRNGKey(0)\n",
    "n_samples = 8\n",
    "dim = 1  # Dimension of the distribution\n",
    "mu_range = (-3.0, 3.0)\n",
    "std_range = (0.1, 5.0)\n",
    "dataset_size = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e0f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset of mu and std values\n",
    "key, subkey1, subkey2 = jax.random.split(key, 3)\n",
    "means = jax.random.uniform(subkey1, (dataset_size, dim), minval=mu_range[0], maxval=mu_range[1])\n",
    "stds_diag = jax.random.uniform(subkey2, (dataset_size, dim), minval=std_range[0], maxval=std_range[1])\n",
    "\n",
    "@jax.jit\n",
    "def true_entropy_batch(means, stds_diag, keys):\n",
    "    def single_entropy(mu, std_diag, key):\n",
    "        samples = mu + std_diag * jax.random.normal(key, (n_samples, dim))\n",
    "        log_det_jacobian = jnp.sum(jnp.log(1 - jnp.tanh(samples) ** 2 + 1e-6), axis=-1)\n",
    "        gauss_entropy = 0.5 * dim * (jnp.log(2 * jnp.pi * jnp.e)) + jnp.sum(jnp.log(std_diag))\n",
    "        return gauss_entropy + jnp.mean(log_det_jacobian)\n",
    "    return jax.vmap(single_entropy)(means, stds_diag, keys)\n",
    "\n",
    "@jax.jit\n",
    "def true_entropy_gradients_batch(means, stds_diag, keys):\n",
    "    def single_entropy(mu, std_diag, key):\n",
    "        samples = mu + std_diag * jax.random.normal(key, (n_samples, dim))\n",
    "        log_det_jacobian = jnp.sum(jnp.log(1 - jnp.tanh(samples) ** 2 + 1e-6), axis=-1)\n",
    "        gauss_entropy = 0.5 * dim * (jnp.log(2 * jnp.pi * jnp.e)) + jnp.sum(jnp.log(std_diag))\n",
    "        entropy = gauss_entropy + jnp.mean(log_det_jacobian)\n",
    "        return entropy\n",
    "    # Compute gradients\n",
    "    grad_mu, grad_std_diag = jax.vmap(jax.grad(single_entropy, argnums=(0, 1)))(means, stds_diag, keys)\n",
    "    return grad_mu, grad_std_diag \n",
    "\n",
    "# Generate keys\n",
    "key, subkey = jax.random.split(key)\n",
    "subkeys = jax.random.split(subkey, dataset_size)\n",
    "true_entropies = true_entropy_batch(means, stds_diag, subkeys)\n",
    "true_entropy_grads_mu, true_entropy_grads_std_diag = true_entropy_gradients_batch(means, stds_diag, subkeys)\n",
    "print(\"mu_grid shape:\", means.shape)\n",
    "print(\"diag_std shape:\", stds_diag.shape)\n",
    "print(\"True entropies shape:\", true_entropies.shape)\n",
    "print(\"True entropy gradients mu shape:\", true_entropy_grads_mu.shape)\n",
    "print(\"True entropy gradients std_diag shape:\", true_entropy_grads_std_diag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a611cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast entropy using learned samples\n",
    "def fast_entropy(mean, std_diag, samples):\n",
    "    x = mean + std_diag * samples\n",
    "    log_det_jacobian = jnp.log(1 - jnp.tanh(x) ** 2 + 1e-6)\n",
    "    gauss_entropy = jnp.sum(jnp.log(std_diag)) + 0.5 * dim * jnp.log(2 * jnp.pi * jnp.e)\n",
    "    return gauss_entropy + jnp.mean(jnp.sum(log_det_jacobian, axis=-1))\n",
    "\n",
    "def fast_entropy_gradients(mean, std_diag, samples):\n",
    "    grad_mu, grad_std_diag = jax.grad(fast_entropy, argnums=(0, 1))(mean, std_diag, samples)\n",
    "    return grad_mu, grad_std_diag\n",
    "\n",
    "# Loss function\n",
    "@jax.jit\n",
    "def loss_fn_entropy_values(samples, means, stds_diag, true_entropies):\n",
    "    def single_loss(mu, std, true_ent):\n",
    "        est = fast_entropy(mu, std, samples)\n",
    "        return (est - true_ent)**2\n",
    "    return jax.vmap(single_loss)(means, stds_diag, true_entropies).mean()\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn_entropy_gradients(samples, means, stds_diag, true_entropy_grads_mu, true_entropy_grads_std_diag):\n",
    "    def single_loss(mu, std, true_ent_grad_mu, true_ent_grad_std_diag):\n",
    "        grad_mu, grad_std_diag = fast_entropy_gradients(mu, std, samples)\n",
    "        return (grad_mu - true_ent_grad_mu)**2 + (grad_std_diag - true_ent_grad_std_diag)**2\n",
    "    return jax.vmap(single_loss)(means, stds_diag, true_entropy_grads_mu, true_entropy_grads_std_diag).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928fd2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "samples = jax.random.normal(subkey, (n_samples,dim))\n",
    "opt = optax.adam(1e-3)\n",
    "opt_state = opt.init(samples)\n",
    "print(\"Sample shape\", samples.shape)\n",
    "\n",
    "# Training loop\n",
    "@jax.jit\n",
    "def train_step(samples, opt_state):\n",
    "    value_loss, grads1 = jax.value_and_grad(loss_fn_entropy_values)(samples, means, stds_diag, true_entropies)\n",
    "    gradient_loss, grads2 = jax.value_and_grad(loss_fn_entropy_gradients)(samples, means, stds_diag, true_entropy_grads_mu, true_entropy_grads_mu)\n",
    "    grads = grads1 + 0*grads2  # Combine gradients for both losses\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    samples = optax.apply_updates(samples, updates)\n",
    "    return samples, opt_state, value_loss, gradient_loss \n",
    "\n",
    "value_losses = []\n",
    "gradient_losses = []\n",
    "for i in tqdm.trange(20_000):\n",
    "    samples, opt_state, loss1, loss2 = train_step(samples, opt_state)\n",
    "    value_losses.append(loss1)\n",
    "    gradient_losses.append(loss2)\n",
    "\n",
    "print(\"Final learned samples (z):\", (samples).sort())\n",
    "plt.plot(value_losses, label='Value Loss')\n",
    "# plt.plot(gradient_losses, label='Gradient Loss')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Steps')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ac07d",
   "metadata": {},
   "source": [
    "### Learned Samples for fast MC entropy estimation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc697ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dim == 1:\n",
    "    samples = samples.squeeze()\n",
    "    print('Half set', [i.item() for i in samples.sort()])\n",
    "    ## Mirror the smaples to make it symmetric around zero\n",
    "    samples = jnp.concatenate([-samples[::-1], samples])  # Adding zero to ensure symmetry\n",
    "    print('Full set', [i.item() for i in samples.sort()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b00d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "[-2.28250789642334, -1.9211180210113525, -1.3525558710098267, -1.0677014589309692, -0.8902221918106079, -0.8044639229774475, -0.7819824814796448, -0.7775918245315552, -0.777317225933075, -0.7008596658706665, -0.6292149424552917, -0.29937660694122314, -0.1358855664730072, -0.09546408802270889, -0.03650583326816559, -0.012498264200985432, 0.012498264200985432, 0.03650583326816559, 0.09546408802270889, 0.1358855664730072, 0.29937660694122314, 0.6292149424552917, 0.7008596658706665, 0.777317225933075, 0.7775918245315552, 0.7819824814796448, 0.8044639229774475, 0.8902221918106079, 1.0677014589309692, 1.3525558710098267, 1.9211180210113525, 2.28250789642334]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed12c0",
   "metadata": {},
   "source": [
    "# Optimal samples for entorpy gradient estimation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f266720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, random\n",
    "import optax\n",
    "\n",
    "# ------------------------------------------\n",
    "# Tanh Gaussian Entropy using Monte Carlo\n",
    "# ------------------------------------------\n",
    "def tanh_gaussian_entropy_mc(samples, mu, log_std):\n",
    "    std = jnp.exp(log_std)\n",
    "    z = mu + samples * std\n",
    "    y = jnp.tanh(z)\n",
    "    log_det_jacobian = jnp.sum(jnp.log(1 - jnp.square(y) + 1e-6), axis=-1)\n",
    "    base_log_prob = -0.5 * jnp.sum(jnp.square(samples) + 2 * log_std + jnp.log(2 * jnp.pi), axis=-1)\n",
    "    return -jnp.mean(base_log_prob - log_det_jacobian)\n",
    "\n",
    "# ------------------------------------------\n",
    "# True Entropy Gradient with Large Sample Set\n",
    "# ------------------------------------------\n",
    "def true_entropy_grad(mu, log_std, key, num_samples=10000):\n",
    "    dim = mu.shape[0]\n",
    "    samples = random.normal(key, shape=(num_samples, dim))\n",
    "    entropy_fn = lambda m, l: tanh_gaussian_entropy_mc(samples, m, l)\n",
    "    grad_mu = grad(entropy_fn, argnums=0)(mu, log_std)\n",
    "    grad_log_std = grad(entropy_fn, argnums=1)(mu, log_std)\n",
    "    return grad_mu, grad_log_std\n",
    "\n",
    "# ------------------------------------------\n",
    "# Estimated Entropy Gradient with Learned Samples\n",
    "# ------------------------------------------\n",
    "def estimated_entropy_grad(samples_learned, mu, log_std):\n",
    "    entropy_fn = lambda m, l: tanh_gaussian_entropy_mc(samples_learned, m, l)\n",
    "    grad_mu = grad(entropy_fn, argnums=0)(mu, log_std)\n",
    "    grad_log_std = grad(entropy_fn, argnums=1)(mu, log_std)\n",
    "    return grad_mu, grad_log_std\n",
    "\n",
    "# ------------------------------------------\n",
    "# Gradient Matching Loss Function\n",
    "# ------------------------------------------\n",
    "def gradient_matching_loss(samples_learned, mu, log_std, key):\n",
    "    true_grad_mu, true_grad_log_std = true_entropy_grad(mu, log_std, key)\n",
    "    est_grad_mu, est_grad_log_std = estimated_entropy_grad(samples_learned, mu, log_std)\n",
    "    loss = jnp.sum((true_grad_mu - est_grad_mu) ** 2) + jnp.sum((true_grad_log_std - est_grad_log_std) ** 2)\n",
    "    return loss\n",
    "\n",
    "# ------------------------------------------\n",
    "# Optimization Loop\n",
    "# ------------------------------------------\n",
    "def optimize_entropy_samples(dim=3, n_learned_samples=16, n_iters=1000, lr=1e-2, seed=0):\n",
    "    key = random.PRNGKey(seed)\n",
    "    mu = jnp.zeros((dim,))\n",
    "    log_std = jnp.zeros((dim,))\n",
    "    key, subkey = random.split(key)\n",
    "    samples_learned = random.normal(subkey, shape=(n_learned_samples, dim))\n",
    "\n",
    "    optimizer = optax.adam(lr)\n",
    "    opt_state = optimizer.init(samples_learned)\n",
    "\n",
    "    @jax.jit\n",
    "    def step(samples, opt_state, key):\n",
    "        loss, grads = jax.value_and_grad(gradient_matching_loss)(samples, mu, log_std, key)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, samples)\n",
    "        samples = optax.apply_updates(samples, updates)\n",
    "        return samples, opt_state, loss\n",
    "\n",
    "    losses = []\n",
    "    for i in range(n_iters):\n",
    "        key, subkey = random.split(key)\n",
    "        samples_learned, opt_state, loss = step(samples_learned, opt_state, subkey)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return samples_learned, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78fecb7",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46068251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "key = jax.random.PRNGKey(7)\n",
    "samples = jax.random.normal(key, shape=(1_000_000,), dtype=jnp.float64)  # Example learned samples\n",
    "samples = jnp.concatenate([-samples[::-1], samples, jnp.array([0])])  # Adding zero to ensure symmetry\n",
    "samples = (samples - jnp.mean(samples)) / jnp.std(samples)  # Normalize samples\n",
    "\n",
    "def f(x):\n",
    "    return jnp.mean(jnp.log1p(-jnp.tanh(x * samples) ** 2)) + 0.5 * jnp.log(2 *jnp.pi *jnp.e * x**2)\n",
    "def f_grad(x):\n",
    "    return (1/x) + jnp.mean(-2 * jnp.tanh(x * samples) * samples)\n",
    "\n",
    "@jax.jit\n",
    "def update_f(x):\n",
    "    x += 0.002*f_grad(x)\n",
    "    return x\n",
    "\n",
    "x_0 = 0.8744  # Initial value for x, very close to the optimal value\n",
    "x_0 = jnp.array(x_0, dtype=jnp.float64)  # Ensure x_0 is a float64 for higher precision\n",
    "\n",
    "for i in range(10_000):\n",
    "    x_0 = update_f(x_0)\n",
    "    if i%100==0:\n",
    "        print(f\"Iteration {i+1}, x: {x_0}\")\n",
    "print(\"Optimized x:\", x_0)\n",
    "print(\"Entropy value:\", f(x_0))\n",
    "print(\"Gradient value:\", f_grad(x_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2906781",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8727344274520874 # 100\n",
    "0.8727468252182007 # 100k\n",
    "0.8743883967399597 # 10M\n",
    "0.8744339942932129 # 100M\n",
    "0.874382734298706  # 100M\n",
    "0.8744062185287476 # 100M\n",
    "0.8744085431098938 # 100M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca05da44",
   "metadata": {},
   "source": [
    "# ModernGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13cc87fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(inf, dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "jnp.arctanh(jnp.tanh(9))  # Should be close to 0.8744085431098938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9543ecda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.isclose(0.999999, 1.0)\n",
    "# jnp.arctanh(0.999999)\n",
    "# jnp.log(1 - jnp.square(0.999999) + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ccd493d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1., 1.],\n",
       "       [1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = jnp.array(1.0)\n",
    "a = jax.lax.cond(\n",
    "    True,\n",
    "    lambda: jnp.ones((2,2)),\n",
    "    lambda: jnp.zeros((2, 2)),  # Fallback to zero if log_prob is not finite,\n",
    ")\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
