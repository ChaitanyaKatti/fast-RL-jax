{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff735c24",
   "metadata": {},
   "source": [
    "# PPO Clip Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8743618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters (example values similar to PPO paper)\n",
    "mu_old = 0.0           # old policy mean\n",
    "log_std_old = 0.0      # old policy log std (σ_old = 1)\n",
    "sigma_old = np.exp(log_std_old)\n",
    "\n",
    "a_t = 0.0              # action taken\n",
    "eps = 0.2              # PPO clip epsilon\n",
    "\n",
    "# Grid for μ_t and log σ_t\n",
    "mu_range = np.linspace(-1.2, 1.2, 300)\n",
    "log_std_range = np.linspace(-1.4, 0.5, 300)\n",
    "MU, LOG_STD = np.meshgrid(mu_range, log_std_range)\n",
    "SIGMA = np.exp(LOG_STD)from flax import struct\n",
    "import jax.numpy as jnp\n",
    "from jax import random, vmap\n",
    "\n",
    "# Approach 1: Using __post_init__ (Recommended)\n",
    "@struct.dataclass\n",
    "class CrazyflieParams:\n",
    "    num_agents: int = 1\n",
    "    mass: float = 0.028\n",
    "    inertia: jnp.ndarray = jnp.array([\n",
    "        [16.571710, 0.830806, 0.718277],\n",
    "        [0.830806, 16.655602, 1.800197],\n",
    "        [0.718277, 1.800197, 29.261652],\n",
    "    ])\n",
    "    arm_length: float = 0.5\n",
    "    g: float = 9.8\n",
    "    motor_settling_time: float = 0.01\n",
    "    \n",
    "    # This will be computed in __post_init__\n",
    "    motor_alpha: float = struct.field(init=False)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # Compute dependent values\n",
    "        motor_alpha = 1 - jnp.exp(-self.motor_settling_time / 0.01)\n",
    "        # Use object.__setattr__ since dataclass is frozen\n",
    "        object.__setattr__(self, 'motor_alpha', motor_alpha)\n",
    "\n",
    "\n",
    "# Compute log r_t\n",
    "log_r = (log_std_old - LOG_STD) + \\\n",
    "        ((a_t - mu_old)**2) / (2 * sigma_old**2) - \\\n",
    "        ((a_t - MU)**2) / (2 * SIGMA**2)\n",
    "\n",
    "# Clip bounds in log space\n",
    "lower_bound = np.log(1 - eps)\n",
    "upper_bound = np.log(1 + eps)\n",
    "\n",
    "# Region mask\n",
    "mask = (log_r >= lower_bound) & (log_r <= upper_bound)\n",
    "\n",
    "# Compute KL divergence D_KL( pi_old || pi_new ) for Gaussian policies\n",
    "KL = np.log(SIGMA / sigma_old) + \\\n",
    "     (sigma_old**2 + (MU - mu_old)**2) / (2 * SIGMA**2) - 0.5\n",
    "\n",
    "# Plot with KL contours\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.contour(MU, LOG_STD, log_r, levels=[lower_bound, upper_bound],\n",
    "            colors='k', linestyles='--', linewidths=1.5, alpha=0.8)\n",
    "plt.imshow(1-mask, extent=[mu_range.min(), mu_range.max(), log_std_range.min(), log_std_range.max()],\n",
    "           origin='lower', alpha=0.15, cmap='gray')\n",
    "\n",
    "# KL divergence contours\n",
    "kl_levels = [0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0]\n",
    "contours = plt.contour(MU, LOG_STD, KL, levels=kl_levels, cmap='coolwarm')\n",
    "plt.clabel(contours, inline=True, fontsize=8, fmt=\"%.2f\")\n",
    "\n",
    "# Mark theta_old\n",
    "plt.scatter(mu_old, log_std_old, color='red', label=r'$\\theta_{\\mathrm{old}}$')\n",
    "plt.xlabel(r'$\\mu_t$')\n",
    "plt.ylabel(r'$\\log \\Sigma_t$')\n",
    "plt.legend()\n",
    "plt.title(\"PPO Clipping Region with KL Divergence Contours\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5db7a6",
   "metadata": {},
   "source": [
    "# Max Tanh Gaussian Distribution Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65047811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1284e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_range = np.linspace(-3, 3, 300)\n",
    "log_std_range = np.linspace(-4, 2, 300)\n",
    "MU, LOG_STD = np.meshgrid(mu_range, log_std_range)\n",
    "SIGMA = np.exp(LOG_STD)\n",
    "\n",
    "def gaussian_pdf(x, mu, log_std):\n",
    "    sigma = np.exp(log_std)\n",
    "    z = (x - mu) / sigma\n",
    "    return 0.5 * np.exp(-0.5 * z**2) / (sigma * np.sqrt(2 * np.pi))\n",
    "\n",
    "def tanh_gaussian_pdf(x, mu, log_std):\n",
    "    return gaussian_pdf(np.arctanh(x), mu, log_std) / (1 - x**2)\n",
    "\n",
    "def entropy_tanh_gaussian(mu, log_std):\n",
    "    N, M = mu.shape\n",
    "    assert N == M, \"MU and LOG_STD must be square matrices of the same shape\"\n",
    "    \n",
    "    K = 1000  # Number of points for numerical integration\n",
    "    x = np.linspace(-1.0 + 1e-6, 1.0 - 1e-6, K)\n",
    "    \n",
    "    MU_exp = mu[..., np.newaxis]            # (N, N, 1)\n",
    "    LOG_STD_exp = log_std[..., np.newaxis]  # (N, N, 1)\n",
    "    x_exp = x[np.newaxis, np.newaxis, :]    # (1, 1, K)\n",
    "\n",
    "    MU_broadcast = np.broadcast_to(MU_exp, (N, N, K))\n",
    "    LOG_STD_broadcast = np.broadcast_to(LOG_STD_exp, (N, N, K))\n",
    "    x_broadcast = np.broadcast_to(x_exp, (N, N, K))\n",
    "    \n",
    "    pdf_values = tanh_gaussian_pdf(x_broadcast, MU_broadcast, LOG_STD_broadcast)\n",
    "\n",
    "    # Entropy is calculated as -∫ p(x) log(p(x)) dx\n",
    "    entropy = -np.sum(pdf_values * np.log(pdf_values + 1e-10), axis=-1) * (x[1] - x[0])\n",
    "    return entropy\n",
    "\n",
    "entropy_values = entropy_tanh_gaussian(MU, LOG_STD)\n",
    "log_sigma_for_max_entropy = log_std_range[np.argmax(entropy_values, axis=0)]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(MU, LOG_STD, entropy_values, levels=100, cmap='viridis', alpha=0.8)\n",
    "plt.plot(MU[0, :], log_sigma_for_max_entropy, color='blue', label='Max Entropy Curve')\n",
    "plt.colorbar(label='Entropy')\n",
    "plt.xlabel(r'$\\mu_t$')\n",
    "plt.ylabel(r'$\\log \\Sigma_t$')\n",
    "plt.title('Entropy of Tanh Gaussian Distribution')\n",
    "plt.scatter(0.0, 0.0, color='red', label=r'$\\theta_{\\mathrm{old}}$', s=50)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04cc23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_values.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abc24f2",
   "metadata": {},
   "source": [
    "# Opimal Parameters Fast Entorpy\n",
    "\n",
    "- k-dimensional\n",
    "- N parameters\n",
    "- Optimized for gradients of entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "376e33b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc4bdcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize samples\n",
    "key = jax.random.PRNGKey(0)\n",
    "n_samples = 8\n",
    "dim = 1  # Dimension of the distribution\n",
    "mu_range = (-3.0, 3.0)\n",
    "std_range = (0.1, 5.0)\n",
    "dataset_size = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42e0f5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu_grid shape: (4096, 1)\n",
      "diag_std shape: (4096, 1)\n",
      "True entropies shape: (4096,)\n",
      "True entropy gradients mu shape: (4096, 1)\n",
      "True entropy gradients std_diag shape: (4096, 1)\n"
     ]
    }
   ],
   "source": [
    "# Generate a dataset of mu and std values\n",
    "key, subkey1, subkey2 = jax.random.split(key, 3)\n",
    "means = jax.random.uniform(subkey1, (dataset_size, dim), minval=mu_range[0], maxval=mu_range[1])\n",
    "stds_diag = jax.random.uniform(subkey2, (dataset_size, dim), minval=std_range[0], maxval=std_range[1])\n",
    "\n",
    "@jax.jit\n",
    "def true_entropy_batch(means, stds_diag, keys):\n",
    "    def single_entropy(mu, std_diag, key):\n",
    "        samples = mu + std_diag * jax.random.normal(key, (n_samples, dim))\n",
    "        log_det_jacobian = jnp.sum(jnp.log(1 - jnp.tanh(samples) ** 2 + 1e-6), axis=-1)\n",
    "        gauss_entropy = 0.5 * dim * (jnp.log(2 * jnp.pi * jnp.e)) + jnp.sum(jnp.log(std_diag))\n",
    "        return gauss_entropy + jnp.mean(log_det_jacobian)\n",
    "    return jax.vmap(single_entropy)(means, stds_diag, keys)\n",
    "\n",
    "@jax.jit\n",
    "def true_entropy_gradients_batch(means, stds_diag, keys):\n",
    "    def single_entropy(mu, std_diag, key):\n",
    "        samples = mu + std_diag * jax.random.normal(key, (n_samples, dim))\n",
    "        log_det_jacobian = jnp.sum(jnp.log(1 - jnp.tanh(samples) ** 2 + 1e-6), axis=-1)\n",
    "        gauss_entropy = 0.5 * dim * (jnp.log(2 * jnp.pi * jnp.e)) + jnp.sum(jnp.log(std_diag))\n",
    "        entropy = gauss_entropy + jnp.mean(log_det_jacobian)\n",
    "        return entropy\n",
    "    # Compute gradients\n",
    "    grad_mu, grad_std_diag = jax.vmap(jax.grad(single_entropy, argnums=(0, 1)))(means, stds_diag, keys)\n",
    "    return grad_mu, grad_std_diag \n",
    "\n",
    "# Generate keys\n",
    "key, subkey = jax.random.split(key)\n",
    "subkeys = jax.random.split(subkey, dataset_size)\n",
    "true_entropies = true_entropy_batch(means, stds_diag, subkeys)\n",
    "true_entropy_grads_mu, true_entropy_grads_std_diag = true_entropy_gradients_batch(means, stds_diag, subkeys)\n",
    "print(\"mu_grid shape:\", means.shape)\n",
    "print(\"diag_std shape:\", stds_diag.shape)\n",
    "print(\"True entropies shape:\", true_entropies.shape)\n",
    "print(\"True entropy gradients mu shape:\", true_entropy_grads_mu.shape)\n",
    "print(\"True entropy gradients std_diag shape:\", true_entropy_grads_std_diag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61a611cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast entropy using learned samples\n",
    "def fast_entropy(mean, std_diag, samples):\n",
    "    x = mean + std_diag * samples\n",
    "    log_det_jacobian = jnp.log(1 - jnp.tanh(x) ** 2 + 1e-6)\n",
    "    gauss_entropy = jnp.sum(jnp.log(std_diag)) + 0.5 * dim * jnp.log(2 * jnp.pi * jnp.e)\n",
    "    return gauss_entropy + jnp.mean(jnp.sum(log_det_jacobian, axis=-1))\n",
    "\n",
    "def fast_entropy_gradients(mean, std_diag, samples):\n",
    "    grad_mu, grad_std_diag = jax.grad(fast_entropy, argnums=(0, 1))(mean, std_diag, samples)\n",
    "    return grad_mu, grad_std_diag\n",
    "\n",
    "# Loss function\n",
    "@jax.jit\n",
    "def loss_fn_entropy_values(samples, means, stds_diag, true_entropies):\n",
    "    def single_loss(mu, std, true_ent):\n",
    "        est = fast_entropy(mu, std, samples)\n",
    "        return (est - true_ent)**2\n",
    "    return jax.vmap(single_loss)(means, stds_diag, true_entropies).mean()\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn_entropy_gradients(samples, means, stds_diag, true_entropy_grads_mu, true_entropy_grads_std_diag):\n",
    "    def single_loss(mu, std, true_ent_grad_mu, true_ent_grad_std_diag):\n",
    "        grad_mu, grad_std_diag = fast_entropy_gradients(mu, std, samples)\n",
    "        return (grad_mu - true_ent_grad_mu)**2 + (grad_std_diag - true_ent_grad_std_diag)**2\n",
    "    return jax.vmap(single_loss)(means, stds_diag, true_entropy_grads_mu, true_entropy_grads_std_diag).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "928fd2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shape (8, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:02<00:00, 6931.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final learned samples (z): [[ 0.1073949 ]\n",
      " [-1.2608293 ]\n",
      " [ 0.5797289 ]\n",
      " [ 0.5797148 ]\n",
      " [ 0.10739301]\n",
      " [-0.45145652]\n",
      " [ 1.8561625 ]\n",
      " [-1.2133342 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUs9JREFUeJzt3Xd8FGXiBvBnNtmSzWY3CaSTBAihh2JooXugARET8ASRO4KiKMKpvxPlOAvF84JgPfVQTwUb0qR4CCIt0hWQDuYAQ0JJQk3vu+/vj7ADawoQdmeS5fl+PvuRnX1n9p3MJvv4tpGEEAJEREREbkKjdgWIiIiInInhhoiIiNwKww0RERG5FYYbIiIicisMN0RERORWGG6IiIjIrTDcEBERkVthuCEiIiK3wnBDREREboXhhqieGzt2LJo2bVqnfadPnw5JkpxbISKieo7hhqiOJEm6oUdKSoraVVXF2LFjYTKZ1K7GDRFC4IsvvkDfvn3h6+sLo9GImJgYzJw5E4WFhWpXr1pbt27F4MGDERYWBoPBgIiICAwdOhQLFiyQyxQVFWH69Om37WeQbl8S7y1FVDdffvmlw/PPP/8c69atwxdffOGw/a677kJQUFCd36e8vBw2mw16vf6m962oqEBFRQUMBkOd37+uxo4di6VLl6KgoEDx974ZVqsVDz30EBYvXow+ffpg+PDhMBqN2LJlCxYsWIC2bdti/fr1t3QNnW3JkiUYOXIkOnXqhAcffBB+fn5IS0vD5s2bodVqsWnTJgDAhQsXEBAQgGnTpmH69OnqVppIQZ5qV4CoofrTn/7k8Hznzp1Yt25dle2/V1RUBKPReMPvo9Vq61Q/APD09ISnJ3/NazN79mwsXrwYkydPxpw5c+Tt48ePx4gRI5CYmIixY8dizZo1itarts/J9OnT0bZtW+zcuRM6nc7htXPnzilRPaJ6jd1SRC7Uv39/tG/fHnv27EHfvn1hNBrx97//HQCwcuVKDBkyBKGhodDr9YiKisIrr7wCq9XqcIzfj7k5efIkJEnC66+/jo8++ghRUVHQ6/Xo2rUrdu3a5bBvdWNuJEnCpEmTsGLFCrRv3x56vR7t2rXD999/X6X+KSkp6NKlCwwGA6KiovDhhx86fRzPkiVLEBsbCy8vLzRu3Bh/+tOfcObMGYcyWVlZePjhh9GkSRPo9XqEhIQgISEBJ0+elMvs3r0b8fHxaNy4Mby8vNCsWTM88sgjtb53cXEx5syZg5YtWyI5ObnK60OHDkVSUhK+//577Ny5EwBw7733onnz5tUeLy4uDl26dHHY9uWXX8rn5+/vjwcffBCnTp1yKFPb56Q6J06cQNeuXasEGwAIDAwEUPk5CQgIAADMmDFD7ia9tgXn119/xR//+Ef4+/vDYDCgS5cu+Pbbbx2ON3/+fEiShM2bN+Pxxx9Ho0aNYDabMWbMGFy+fNmhbF2uAZEr8H/piFzs4sWLGDx4MB588EH86U9/krs35s+fD5PJhL/+9a8wmUzYuHEjXn75ZeTl5Tm0INRkwYIFyM/Px+OPPw5JkjB79mwMHz4cv/3223Vbe7Zu3Yply5bhySefhI+PD/71r3/h/vvvR0ZGBho1agQA2Lt3LwYNGoSQkBDMmDEDVqsVM2fOlL8wnWH+/Pl4+OGH0bVrVyQnJyM7OxvvvPMOtm3bhr1798LX1xcAcP/99+Pw4cP4y1/+gqZNm+LcuXNYt24dMjIy5Od33303AgIC8Le//Q2+vr44efIkli1bdt2fw+XLl/H000/X2MI1ZswYzJs3D6tWrUKPHj0wcuRIjBkzBrt27ULXrl3lcunp6di5c6fDtXv11Vfx0ksvYcSIEXj00Udx/vx5vPvuu+jbt6/D+QE1f06qExkZiQ0bNuD06dNo0qRJtWUCAgIwd+5cTJgwAcOGDcPw4cMBAB06dAAAHD58GL169UJYWBj+9re/wdvbG4sXL0ZiYiK++eYbDBs2zOF4kyZNgq+vL6ZPn47U1FTMnTsX6enpSElJgSRJdb4GRC4hiMgpJk6cKH7/K9WvXz8BQHzwwQdVyhcVFVXZ9vjjjwuj0ShKSkrkbUlJSSIyMlJ+npaWJgCIRo0aiUuXLsnbV65cKQCI//73v/K2adOmVakTAKHT6cTx48flbfv37xcAxLvvvitvGzp0qDAajeLMmTPytmPHjglPT88qx6xOUlKS8Pb2rvH1srIyERgYKNq3by+Ki4vl7atWrRIAxMsvvyyEEOLy5csCgJgzZ06Nx1q+fLkAIHbt2nXdel3r7bffFgDE8uXLayxz6dIlAUAMHz5cCCFEbm6u0Ov14tlnn3UoN3v2bCFJkkhPTxdCCHHy5Enh4eEhXn31VYdyBw8eFJ6eng7ba/ucVOeTTz6Rr+Odd94pXnrpJbFlyxZhtVodyp0/f14AENOmTatyjAEDBoiYmBiHz5rNZhM9e/YU0dHR8rZ58+YJACI2NlaUlZU5nC8AsXLlSiFE3a8BkSuwW4rIxfR6PR5++OEq2728vOR/5+fn48KFC+jTpw+Kiorw66+/Xve4I0eOhJ+fn/y8T58+AIDffvvtuvsOHDgQUVFR8vMOHTrAbDbL+1qtVqxfvx6JiYkIDQ2Vy7Vo0QKDBw++7vFvxO7du3Hu3Dk8+eSTDgOehwwZgtatW+O7774DUPlz0ul0SElJqdINYmdvAVm1ahXKy8tvuA75+fkAAB8fnxrL2F/Ly8sDAJjNZgwePBiLFy+GuGY+xqJFi9CjRw9EREQAAJYtWwabzYYRI0bgwoUL8iM4OBjR0dHyoF+7mj4n1XnkkUfw/fffo3///ti6dSteeeUV9OnTB9HR0di+fft197906RI2btyIESNGyJ+9Cxcu4OLFi4iPj8exY8eqdA2OHz/eoUVwwoQJ8PT0xOrVqwHU/RoQuQLDDZGLhYWFVTs24vDhwxg2bBgsFgvMZjMCAgLkwci5ubnXPa79S9TOHnRqCgC17Wvf377vuXPnUFxcjBYtWlQpV922ukhPTwcAtGrVqsprrVu3ll/X6/V47bXXsGbNGgQFBaFv376YPXs2srKy5PL9+vXD/fffjxkzZqBx48ZISEjAvHnzUFpaWmsd7MHFHnKqU10AGjlyJE6dOoUdO3YAqBwDs2fPHowcOVIuc+zYMQghEB0djYCAAIfH0aNHqwz8relzUpP4+HisXbsWOTk52Lx5MyZOnIj09HTce++91x1UfPz4cQgh8NJLL1Wp27Rp0wBUHZgcHR3t8NxkMiEkJEQe91TXa0DkChxzQ+Ri17bQ2OXk5KBfv34wm82YOXMmoqKiYDAY8Msvv2DKlCmw2WzXPa6Hh0e128UNrO5wK/uq4ZlnnsHQoUOxYsUKrF27Fi+99BKSk5OxceNGdO7cGZIkYenSpdi5cyf++9//Yu3atXjkkUfwxhtvYOfOnTWut9OmTRsAwIEDB5CYmFhtmQMHDgAA2rZtK28bOnQojEYjFi9ejJ49e2Lx4sXQaDR44IEH5DI2mw2SJGHNmjXV/rx/X6fqPic3wmg0ok+fPujTpw8aN26MGTNmYM2aNUhKSqpxH/vna/LkyYiPj6+2zM2G2LpeAyJXYLghUkFKSgouXryIZcuWoW/fvvL2tLQ0FWt1VWBgIAwGA44fP17lteq21UVkZCQAIDU1FX/4wx8cXktNTZVft4uKisKzzz6LZ599FseOHUOnTp3wxhtvOKw31KNHD/To0QOvvvoqFixYgNGjR2PhwoV49NFHq61D79694evriwULFuCFF16oNoR8/vnnACpnSdl5e3vj3nvvxZIlS/Dmm29i0aJF6NOnj0MXXlRUFIQQaNasGVq2bHmTP526sc/UyszMBIAaZ7XZZ3tptVoMHDjwho597Ngx3HnnnfLzgoICZGZm4p577nEod7PXgMgV2C1FpAL7l+i1LSVlZWX497//rVaVHHh4eGDgwIFYsWIFzp49K28/fvy409Z76dKlCwIDA/HBBx84dF2sWbMGR48exZAhQwBUrvdSUlLisG9UVBR8fHzk/S5fvlyl1alTp04AUGu3iNFoxOTJk5GamooXXnihyuvfffcd5s+fj/j4ePTo0cPhtZEjR+Ls2bP4+OOPsX//focuKQAYPnw4PDw8MGPGjCp1E0Lg4sWLNdbrejZs2FDtdvv4F3tXn32dnJycHIdygYGB6N+/Pz788EM5CF3r/PnzVbZ99NFHDmNp5s6di4qKCnkMVl2vAZErsOWGSAU9e/aEn58fkpKS8NRTT0GSJHzxxRf1qlto+vTp+OGHH9CrVy9MmDABVqsV7733Htq3b499+/bd0DHKy8vxj3/8o8p2f39/PPnkk3jttdfw8MMPo1+/fhg1apQ8Fbxp06b4v//7PwDA//73PwwYMAAjRoxA27Zt4enpieXLlyM7OxsPPvggAOCzzz7Dv//9bwwbNgxRUVHIz8/Hf/7zH5jN5iotC7/3t7/9DXv37sVrr72GHTt24P7774eXlxe2bt2KL7/8Em3atMFnn31WZb977rkHPj4+mDx5Mjw8PHD//fc7vB4VFYV//OMfmDp1Kk6ePInExET4+PggLS0Ny5cvx/jx4zF58uQb+jn+XkJCApo1a4ahQ4ciKioKhYWFWL9+Pf773/+ia9euGDp0KIDKrq62bdti0aJFaNmyJfz9/dG+fXu0b98e77//Pnr37o2YmBg89thjaN68ObKzs7Fjxw6cPn0a+/fvd3jPsrIy+Tqkpqbi3//+N3r37o377rvvlq8BkdOpM0mLyP3UNBW8Xbt21Zbftm2b6NGjh/Dy8hKhoaHi+eefF2vXrhUAxKZNm+RyNU0Fr25qNH437bemqeATJ06ssm9kZKRISkpy2LZhwwbRuXNnodPpRFRUlPj444/Fs88+KwwGQw0/hauSkpIEgGofUVFRcrlFixaJzp07C71eL/z9/cXo0aPF6dOn5dcvXLggJk6cKFq3bi28vb2FxWIR3bt3F4sXL5bL/PLLL2LUqFEiIiJC6PV6ERgYKO69916xe/fu69ZTCCGsVquYN2+e6NWrlzCbzcJgMIh27dqJGTNmiIKCghr3Gz16tAAgBg4cWGOZb775RvTu3Vt4e3sLb29v0bp1azFx4kSRmpoql6ntc1Kdr7/+Wjz44IMiKipKeHl5CYPBINq2bSteeOEFkZeX51B2+/btIjY2Vuh0uiqfjxMnTogxY8aI4OBgodVqRVhYmLj33nvF0qVL5TL2qeA//vijGD9+vPDz8xMmk0mMHj1aXLx4US53q9eAyJl4bykiuimJiYk4fPgwjh07pnZVSAH2hRZ37dpVZfVlovqKY26IqEbFxcUOz48dO4bVq1ejf//+6lSIiOgGcMwNEdWoefPmGDt2LJo3b4709HTMnTsXOp0Ozz//vNpVIyKqEcMNEdVo0KBB+Prrr5GVlQW9Xo+4uDj885//rLKgGxFRfcIxN0RERORWOOaGiIiI3ArDDREREbmV227Mjc1mw9mzZ+Hj41Pj0uRERERUvwghkJ+fj9DQUGg0tbfN3Hbh5uzZswgPD1e7GkRERFQHp06dQpMmTWotc9uFGx8fHwCVPxyz2axybYiIiOhG5OXlITw8XP4er81tF27sXVFms5nhhoiIqIG5kSElHFBMREREboXhhoiIiNwKww0RERG5ldtuzA0RETVcNpsNZWVlaleDXESn0113mveNYLghIqIGoaysDGlpabDZbGpXhVxEo9GgWbNm0Ol0t3QchhsiIqr3hBDIzMyEh4cHwsPDnfJ/91S/2BfZzczMRERExC0ttMtwQ0RE9V5FRQWKiooQGhoKo9GodnXIRQICAnD27FlUVFRAq9XW+TiMvkREVO9ZrVYAuOXuCqrf7NfXfr3riuGGiIgaDN4T0L056/oy3BAREZFbYbghIiKqx/r3749nnnlG7Wo0KAw3RERELjB06FAMGjSo2te2bNkCSZJw4MABhWtVvZMnT0KSJOzbt0/tqjgFw42TlFZYcTanGGdzitWuChER1QPjxo3DunXrcPr06SqvzZs3D126dEGHDh1UqJn7Y7hxkoOnc9Fz1kY89J+daleFiIjqgXvvvRcBAQGYP3++w/aCggIsWbIE48aNw8WLFzFq1CiEhYXBaDQiJiYGX3/9da3HlSQJK1ascNjm6+vr8D6nTp3CiBEj4OvrC39/fyQkJODkyZN1PpfS0lI89dRTCAwMhMFgQO/evbFr1y759cuXL2P06NEICAiAl5cXoqOjMW/ePACViy9OmjQJISEhMBgMiIyMRHJycp3rciMYbpxE61H5oyy3CpVrQkTk/oQQKCqrUOUhxI39nff09MSYMWMwf/58h32WLFkCq9WKUaNGoaSkBLGxsfjuu+9w6NAhjB8/Hn/+85/x888/1/lnU15ejvj4ePj4+GDLli3Ytm0bTCYTBg0aVOdbVzz//PP45ptv8Nlnn+GXX35BixYtEB8fj0uXLgEAXnrpJRw5cgRr1qzB0aNHMXfuXDRu3BgA8K9//QvffvstFi9ejNTUVHz11Vdo2rRpnc/vRnARPyfReVaGmzIrlwUnInK14nIr2r68VpX3PjIzHkbdjX19PvLII5gzZw5+/PFH9O/fH0Bll9T9998Pi8UCi8WCyZMny+X/8pe/YO3atVi8eDG6detWp/otWrQINpsNH3/8sTy1et68efD19UVKSgruvvvumzpeYWEh5s6di/nz52Pw4MEAgP/85z9Yt24dPvnkEzz33HPIyMhA586d0aVLFwBwCC8ZGRmIjo5G7969IUkSIiMj63ReN4MtN05ib7kpq2C4ISKiSq1bt0bPnj3x6aefAgCOHz+OLVu2YNy4cQAqF6t75ZVXEBMTA39/f5hMJqxduxYZGRl1fs/9+/fj+PHj8PHxgclkgslkgr+/P0pKSnDixImbPt6JEydQXl6OXr16ydu0Wi26deuGo0ePAgAmTJiAhQsXolOnTnj++eexfft2uezYsWOxb98+tGrVCk899RR++OGHOp/bjWLLjZPo5G4phhsiIlfz0nrgyMx41d77ZowbNw5/+ctf8P7772PevHmIiopCv379AABz5szBO++8g7fffhsxMTHw9vbGM888U2v3kSRJVbrGysvL5X8XFBQgNjYWX331VZV9AwICbqruN2rw4MFIT0/H6tWrsW7dOgwYMAATJ07E66+/jjvuuANpaWlYs2YN1q9fjxEjRmDgwIFYunSpS+oCMNw4jb1biuGGiMj1JEm64a4htY0YMQJPP/00FixYgM8//xwTJkyQu4u2bduGhIQE/OlPfwJQefPI//3vf2jbtm2NxwsICEBmZqb8/NixYygqKpKf33HHHVi0aBECAwNhNptvuf5RUVHQ6XTYtm2b3KVUXl6OXbt2Oay/ExAQgKSkJCQlJaFPnz547rnn8PrrrwMAzGYzRo4ciZEjR+KPf/wjBg0ahEuXLsHf3/+W61edhvHJaAC0HpUf1HKrgM0moNFwiXAiIgJMJhNGjhyJqVOnIi8vD2PHjpVfi46OxtKlS7F9+3b4+fnhzTffRHZ2dq3h5g9/+APee+89xMXFwWq1YsqUKQ43mRw9ejTmzJmDhIQEzJw5E02aNEF6ejqWLVuG559/Hk2aNKnx2KmpqVW2tWvXDhMmTMBzzz0Hf39/REREYPbs2SgqKpK7115++WXExsaiXbt2KC0txapVq9CmTRsAwJtvvomQkBB07twZGo0GS5YsQXBwMHx9fW/yJ3njGG6cROt5dfhSuc0Gvebmmi2JiMh9jRs3Dp988gnuuecehIaGyttffPFF/Pbbb4iPj4fRaMT48eORmJiI3NzcGo/1xhtv4OGHH0afPn0QGhqKd955B3v27JFfNxqN2Lx5M6ZMmYLhw4cjPz8fYWFhGDBgwHVbch588MEq206dOoVZs2bBZrPhz3/+M/Lz89GlSxesXbsWfn5+ACpveDl16lScPHkSXl5e6NOnDxYuXAgA8PHxwezZs3Hs2DF4eHiga9euWL16NTQa1w37lcSNzmlzE3l5ebBYLMjNzXVKc51dSbkVrV/6HgBwaEY8THrmRiIiZykpKUFaWhqaNWsGg8GgdnXIRWq7zjfz/c3ZUk5iny0FcMYUERGRmhhunMRDI8FDYx93w3BDRESkFoYbJ9JxrRsiIiLVMdw4kX3GFFcpJiIiUg/DjRNxrRsiIte6zebA3HacdX0ZbpxIXqW4gr98RETO5OFRubxGXW/8SA2D/frar3ddcb6yE2nlm2daVa4JEZF78fT0hNFoxPnz56HVal26Rgqpw2az4fz58zAajfD0vLV4wnDjRFdvnsmWGyIiZ5IkCSEhIUhLS0N6erra1SEX0Wg0iIiIkG9PUVcMN07Em2cSEbmOTqdDdHQ0u6bcmE6nc0qrHMONE8ndUpwKTkTkEhqNhisU03Wx09KJdB5cxI+IiEhtDDdOpJMHFDPcEBERqYXhxom0XKGYiIhIdQw3TqSVBxRzthQREZFaGG6ciCsUExERqY/hxol440wiIiL1Mdw4EW+cSUREpD6GGyditxQREZH6GG6ciLOliIiI1Mdw40S8/QIREZH6GG6c6Gq3FKeCExERqYXhxons3VKl7JYiIiJSDcONE2nZLUVERKQ6hhsn4mwpIiIi9THcOJH9ruCcLUVERKQehhsnYrcUERGR+hhunMjeLVXG2VJERESqYbhxoquL+FlVrgkREdHti+HGia52S7HlhoiISC0MN06k52wpIiIi1THcOBHvLUVERKQ+hhsn0tqngrPlhoiISDUMN07ERfyIiIjUx3DjROyWIiIiUp+q4Wbu3Lno0KEDzGYzzGYz4uLisGbNmhrLz58/H5IkOTwMBoOCNa4d7wpORESkPk8137xJkyaYNWsWoqOjIYTAZ599hoSEBOzduxft2rWrdh+z2YzU1FT5uSRJSlX3unT2qeBsuSEiIlKNquFm6NChDs9fffVVzJ07Fzt37qwx3EiShODgYCWqd9O0V1puSjnmhoiISDX1ZsyN1WrFwoULUVhYiLi4uBrLFRQUIDIyEuHh4UhISMDhw4drPW5paSny8vIcHq5iny1VbrVBCHZNERERqUH1cHPw4EGYTCbo9Xo88cQTWL58Odq2bVtt2VatWuHTTz/FypUr8eWXX8Jms6Fnz544ffp0jcdPTk6GxWKRH+Hh4a46Feg9PAAAQgBWG8MNERGRGiShchNDWVkZMjIykJubi6VLl+Ljjz/Gjz/+WGPAuVZ5eTnatGmDUaNG4ZVXXqm2TGlpKUpLS+XneXl5CA8PR25uLsxms9POAwCKyirQ9uW1AIAjM+Nh1Kna60dEROQ28vLyYLFYbuj7W/VvX51OhxYtWgAAYmNjsWvXLrzzzjv48MMPr7uvVqtF586dcfz48RrL6PV66PV6p9W31vp4XG0IK68QgE6RtyUiIqJrqN4t9Xs2m82hpaU2VqsVBw8eREhIiItrdWM8NRLsk7e4SjEREZE6VG25mTp1KgYPHoyIiAjk5+djwYIFSElJwdq1lV07Y8aMQVhYGJKTkwEAM2fORI8ePdCiRQvk5ORgzpw5SE9Px6OPPqrmacgkSYLWQ4OyChvDDRERkUpUDTfnzp3DmDFjkJmZCYvFgg4dOmDt2rW46667AAAZGRnQaK42Ll2+fBmPPfYYsrKy4Ofnh9jYWGzfvv2GxucoRXcl3HCtGyIiInWoPqBYaTczIKku7nhlHS4VlmHd//VFdJCP049PRER0O7qZ7+96N+amobOvdVPKlhsiIiJVMNw4mX3GFO8MTkREpA6GGyfjzTOJiIjUxXDjZPabZ5axW4qIiEgVDDdOxm4pIiIidTHcOJm9W4rr3BAREamD4cbJ7LOl2C1FRESkDoYbJ2O3FBERkboYbpxM78lwQ0REpCaGGyfTcrYUERGRqhhunEwON1znhoiISBUMN06mY7cUERGRqhhunIzdUkREROpiuHEy3ZWp4Gy5ISIiUgfDjZNxET8iIiJ1Mdw4GbuliIiI1MVw42RcxI+IiEhdDDdOJs+WquBUcCIiIjUw3DiZzoNjboiIiNTEcONk8o0zGW6IiIhUwXDjZDpPDwBAOQcUExERqYLhxsnYckNERKQuhhsn4+0XiIiI1MVw42T2AcWcLUVERKQOhhsns69zU8qWGyIiIlUw3DiZVl7nhuGGiIhIDQw3TqbjCsVERESqYrhxMp0nZ0sRERGpieHGyeR7S7FbioiISBUMN05mnwpeZuVsKSIiIjUw3DiZveWmrMKqck2IiIhuTww3TnZ1QDFbboiIiNTAcONkXKGYiIhIXQw3TmbvlqqwCdhsbL0hIiJSGsONk9lvnAlwOjgREZEaGG6czN4tBbBrioiISA0MN06m1Vz9kZZxrRsiIiLFMdw4mUYjwVNT2TXFGVNERETKY7hxAc6YIiIiUg/DjQvYZ0yVsluKiIhIcQw3LqDlncGJiIhUw3DjAnp2SxEREamG4cYF7GvdcLYUERGR8hhuXEC+eSZbboiIiBTHcOMCV2dLcSo4ERGR0hhuXEBuuWG3FBERkeIYblxAx9lSREREqmG4cQEu4kdERKQehhsXsM+W4iJ+REREymO4cQEu4kdERKQehhsXkLul2HJDRESkOIYbF9BxnRsiIiLVMNy4wNVuKa5zQ0REpDSGGxewd0txnRsiIiLlMdy4AG+/QEREpB6GGxfQelZOBeeAYiIiIuUx3LiAnlPBiYiIVMNw4wLsliIiIlIPw40LaOUBxZwtRUREpDRVw83cuXPRoUMHmM1mmM1mxMXFYc2aNbXus2TJErRu3RoGgwExMTFYvXq1QrW9cbxxJhERkXpUDTdNmjTBrFmzsGfPHuzevRt/+MMfkJCQgMOHD1dbfvv27Rg1ahTGjRuHvXv3IjExEYmJiTh06JDCNa+dllPBiYiIVCMJIepV34m/vz/mzJmDcePGVXlt5MiRKCwsxKpVq+RtPXr0QKdOnfDBBx/c0PHz8vJgsViQm5sLs9nstHpfa9GuDEz55iAGtA7EJ2O7uuQ9iIiIbic38/1db8bcWK1WLFy4EIWFhYiLi6u2zI4dOzBw4ECHbfHx8dixY0eNxy0tLUVeXp7Dw9XkRfzYLUVERKQ41cPNwYMHYTKZoNfr8cQTT2D58uVo27ZttWWzsrIQFBTksC0oKAhZWVk1Hj85ORkWi0V+hIeHO7X+1ZFnS7FbioiISHGqh5tWrVph3759+OmnnzBhwgQkJSXhyJEjTjv+1KlTkZubKz9OnTrltGPXRMsBxURERKrxVLsCOp0OLVq0AADExsZi165deOedd/Dhhx9WKRscHIzs7GyHbdnZ2QgODq7x+Hq9Hnq93rmVvg57txRvnElERKQ81Vtufs9ms6G0tLTa1+Li4rBhwwaHbevWratxjI5adOyWIiIiUo2qLTdTp07F4MGDERERgfz8fCxYsAApKSlYu3YtAGDMmDEICwtDcnIyAODpp59Gv3798MYbb2DIkCFYuHAhdu/ejY8++kjN06iC3VJERETqUTXcnDt3DmPGjEFmZiYsFgs6dOiAtWvX4q677gIAZGRkQKO52rjUs2dPLFiwAC+++CL+/ve/Izo6GitWrED79u3VOoVqcbYUERGReurdOjeupsQ6N4fP5mLIv7Yi0EePn18YeP0diIiIqFYNcp0bd8LbLxAREamH4cYFOFuKiIhIPQw3LsBF/IiIiNTDcOMCcrix2nCbDWkiIiJSHcONC9i7pQCgwsZwQ0REpCSGGxewDygG2DVFRESkNIYbF9B6SPK/OWOKiIhIWQw3LuDpoYHmSr7hQn5ERETKYrhxEc6YIiIiUgfDjYtcXciPA4qJiIiUxHDjIlcX8mPLDRERkZIYblyE3VJERETqYLhxEa1n5YhiDigmIiJSFsONi8hjbthyQ0REpCiGGxe59hYMREREpByGGxfhgGIiIiJ1MNy4iE4eUMyp4EREREpiuHERdksRERGpg+HGRbSeHFBMRESkBoYbF7m6QjHDDRERkZIYblxEx3VuiIiIVMFw4yJcoZiIiEgdDDcuwhtnEhERqYPhxkXsA4rZckNERKQshhsX4YBiIiIidTDcuAhXKCYiIlIHw42LaD0qZ0uVsluKiIhIUQw3LqJltxQREZEqGG5chN1SRERE6mC4cREd17khIiJSBcONi2i5zg0REZEqGG5cxN4txdsvEBERKYvhxkV4+wUiIiJ11CncnDp1CqdPn5af//zzz3jmmWfw0UcfOa1iDZ19KjgHFBMRESmrTuHmoYcewqZNmwAAWVlZuOuuu/Dzzz/jhRdewMyZM51awYaKKxQTERGpo07h5tChQ+jWrRsAYPHixWjfvj22b9+Or776CvPnz3dm/RosHe8tRUREpIo6hZvy8nLo9XoAwPr163HfffcBAFq3bo3MzEzn1a4Bk8fccLYUERGRouoUbtq1a4cPPvgAW7Zswbp16zBo0CAAwNmzZ9GoUSOnVrCh4grFRERE6qhTuHnttdfw4Ycfon///hg1ahQ6duwIAPj222/l7qrbHbuliIiI1OFZl5369++PCxcuIC8vD35+fvL28ePHw2g0Oq1yDRkHFBMREamjTi03xcXFKC0tlYNNeno63n77baSmpiIwMNCpFWyotJ6cCk5ERKSGOoWbhIQEfP755wCAnJwcdO/eHW+88QYSExMxd+5cp1awobK33JSyW4qIiEhRdQo3v/zyC/r06QMAWLp0KYKCgpCeno7PP/8c//rXv5xawYaKA4qJiIjUUadwU1RUBB8fHwDADz/8gOHDh0Oj0aBHjx5IT093agUbKvuAYt44k4iISFl1CjctWrTAihUrcOrUKaxduxZ33303AODcuXMwm81OrWBDZe+WstoErDYGHCIiIqXUKdy8/PLLmDx5Mpo2bYpu3bohLi4OQGUrTufOnZ1awYZK63n1R8uuKSIiIuXUaSr4H//4R/Tu3RuZmZnyGjcAMGDAAAwbNsxplWvI7DfOBIAyqw0GrYeKtSEiIrp91CncAEBwcDCCg4Plu4M3adKEC/hdw94tBXAhPyIiIiXVqVvKZrNh5syZsFgsiIyMRGRkJHx9ffHKK6/AZuMXOQBIkiS33rBbioiISDl1arl54YUX8Mknn2DWrFno1asXAGDr1q2YPn06SkpK8Oqrrzq1kg2V1kODcqsV5RUcUExERKSUOoWbzz77DB9//LF8N3AA6NChA8LCwvDkk08y3Fyh89SgqMyKMqtV7aoQERHdNurULXXp0iW0bt26yvbWrVvj0qVLt1wpd2FfyK+MLTdERESKqVO46dixI957770q29977z106NDhlivlLnjzTCIiIuXVqVtq9uzZGDJkCNavXy+vcbNjxw6cOnUKq1evdmoFGzL7KsVlDDdERESKqVPLTb9+/fC///0Pw4YNQ05ODnJycjB8+HAcPnwYX3zxhbPr2GDJs6U4FZyIiEgxdV7nJjQ0tMrA4f379+OTTz7BRx99dMsVcwfymBu23BARESmmTi03dGPkbim23BARESmG4caFtB68MzgREZHSGG5ciLOliIiIlHdTY26GDx9e6+s5OTm3Uhe3w24pIiIi5d1Uy43FYqn1ERkZiTFjxtzw8ZKTk9G1a1f4+PggMDAQiYmJSE1NrXWf+fPnQ5Ikh4fBYLiZ01CMfbYUBxQTEREp56ZabubNm+fUN//xxx8xceJEdO3aFRUVFfj73/+Ou+++G0eOHIG3t3eN+5nNZocQJEmSU+vlLFp2SxERESmuzlPBneH77793eD5//nwEBgZiz5496Nu3b437SZKE4OBgV1fvlrFbioiISHn1akBxbm4uAMDf37/WcgUFBYiMjER4eDgSEhJw+PDhGsuWlpYiLy/P4aEUDigmIiJSXr0JNzabDc888wx69eqF9u3b11iuVatW+PTTT7Fy5Up8+eWXsNls6NmzJ06fPl1t+eTkZIdxQeHh4a46hSquLuLHqeBERERKqTfhZuLEiTh06BAWLlxYa7m4uDiMGTMGnTp1Qr9+/bBs2TIEBATgww8/rLb81KlTkZubKz9OnTrliupXi91SREREylN1zI3dpEmTsGrVKmzevBlNmjS5qX21Wi06d+6M48ePV/u6Xq+HXq93RjVvmtxyw3BDRESkGFVbboQQmDRpEpYvX46NGzeiWbNmN30Mq9WKgwcPIiQkxAU1vDV6+a7gVpVrQkREdPtQteVm4sSJWLBgAVauXAkfHx9kZWUBqFxPx8vLCwAwZswYhIWFITk5GQAwc+ZM9OjRAy1atEBOTg7mzJmD9PR0PProo6qdR00MWg8AQEk5W26IiIiUomq4mTt3LgCgf//+DtvnzZuHsWPHAgAyMjKg0VxtYLp8+TIee+wxZGVlwc/PD7Gxsdi+fTvatm2rVLVvmL3lppTdUkRERIpRNdwIcf1ZRCkpKQ7P33rrLbz11lsuqpFz6bVXwk05u6WIiIiUUm9mS7kjg+eVbim23BARESmG4caF2HJDRESkPIYbF9JfabnhmBsiIiLlMNy4kOFKy00JW26IiIgUw3DjQvaWGy7iR0REpByGGxfiVHAiIiLlMdy40NVF/NgtRUREpBSGGxdiyw0REZHyGG5cSJ4KXsGWGyIiIqUw3LiQfRG/cquA1Xb91ZiJiIjo1jHcuJC95QZg6w0REZFSGG5cyD4VHABKeWdwIiIiRTDcuJCHRoLWQwIAlLDlhoiISBEMNy4m34KBLTdERESKYLhxMU4HJyIiUhbDjYtxIT8iIiJlMdy4GFtuiIiIlMVw42I6Ty7kR0REpCSGGxe72i3FlhsiIiIlMNy4mJ4tN0RERIpiuHExvZZTwYmIiJTEcONihistN1zEj4iISBkMNy7GlhsiIiJlMdy4GKeCExERKYvhxsUMV+4MzkX8iIiIlMFw42KGK/eW4pgbIiIiZTDcuJiX7kq4KWO4ISIiUgLDjYvZF/ErZrcUERGRIhhuXMxLDjccUExERKQEhhsXs3dLFbNbioiISBEMNy7mJd9biuGGiIhICQw3Lia33DDcEBERKYLhxsXkMTfsliIiIlIEw42LseWGiIhIWQw3LsaWGyIiImUx3LgY17khIiJSFsONi7FbioiISFkMNy5mvNJyU1Zhg9UmVK4NERGR+2O4cTF7yw3AtW6IiIiUwHDjYnrPqz9idk0RERG5HsONi0mSxBlTRERECmK4UQAHFRMRESmH4UYBbLkhIiJSDsONAthyQ0REpByGGwV4cSE/IiIixTDcKMAebkrYLUVERORyDDcKMFzplipiuCEiInI5hhsFeGkrf8zsliIiInI9hhsFGHWeALhCMRERkRIYbhRg4FRwIiIixTDcKICzpYiIiJTDcKMALx3H3BARESmF4UYB8lRwhhsiIiKXY7hRgH3MDaeCExERuR7DjQJM+srZUoWlFSrXhIiIyP0x3CjAZKgMN/klDDdERESuxnCjAG97y00Zww0REZGrMdwowOdKuClgyw0REZHLMdwowN4tVcAxN0RERC6narhJTk5G165d4ePjg8DAQCQmJiI1NfW6+y1ZsgStW7eGwWBATEwMVq9erUBt685bx3BDRESkFFXDzY8//oiJEydi586dWLduHcrLy3H33XejsLCwxn22b9+OUaNGYdy4cdi7dy8SExORmJiIQ4cOKVjzm+NjsN9byoYKq03l2hAREbk3SQgh1K6E3fnz5xEYGIgff/wRffv2rbbMyJEjUVhYiFWrVsnbevTogU6dOuGDDz647nvk5eXBYrEgNzcXZrPZaXWvTbnVhugX1gAA9r98NyxGrSLvS0RE5C5u5vu7Xo25yc3NBQD4+/vXWGbHjh0YOHCgw7b4+Hjs2LHDpXW7FVoPDfSelT/q/NJylWtDRETk3jzVroCdzWbDM888g169eqF9+/Y1lsvKykJQUJDDtqCgIGRlZVVbvrS0FKWlpfLzvLw851T4JvkYPFFaUMZxN0RERC5Wb1puJk6ciEOHDmHhwoVOPW5ycjIsFov8CA8Pd+rxb5Q3VykmIiJSRL0IN5MmTcKqVauwadMmNGnSpNaywcHByM7OdtiWnZ2N4ODgastPnToVubm58uPUqVNOq/fNsN+CgasUExERuZaq4UYIgUmTJmH58uXYuHEjmjVrdt194uLisGHDBodt69atQ1xcXLXl9Xo9zGazw0MN9nDDbikiIiLXUnXMzcSJE7FgwQKsXLkSPj4+8rgZi8UCLy8vAMCYMWMQFhaG5ORkAMDTTz+Nfv364Y033sCQIUOwcOFC7N69Gx999JFq53EjePNMIiIiZajacjN37lzk5uaif//+CAkJkR+LFi2Sy2RkZCAzM1N+3rNnTyxYsAAfffQROnbsiKVLl2LFihW1DkKuD3jzTCIiImWo2nJzI0vspKSkVNn2wAMP4IEHHnBBjVzn6oBiq8o1ISIicm/1YkDx7UC+eSbXuSEiInIphhuFcEAxERGRMhhuFOIthxt2SxEREbkSw41C7AOKC0rYLUVERORKDDcKYbcUERGRMhhuFGJitxQREZEiGG4UIndLcbYUERGRSzHcKMTEdW6IiIgUwXCjELlbiisUExERuRTDjULs3VJlVhtKK9h6Q0RE5CoMNwox6TyhkSr/nVvMcTdERESuwnCjEI1Ggq9RBwC4XMhwQ0RE5CoMNwryM2oBAJcKy1SuCRERkftiuFGQn73lpojhhoiIyFUYbhTk581wQ0RE5GoMNwryl8fcMNwQERG5CsONguwtN5c4oJiIiMhlGG4UZB9QzG4pIiIi12G4UdDVlhuGGyIiIldhuFGQfcxNDltuiIiIXIbhRkFyyw3DDRERkcsw3ChIHnPDAcVEREQuw3CjIP8rLTcFpRW8eSYREZGLMNwoyGzQyjfPzCli6w0REZErMNwoSKOReAsGIiIiF2O4UZgvb55JRETkUgw3CvPnWjdEREQuxXCjsEAfAwAgO69U5ZoQERG5J4YbhQWZK8PNubwSlWtCRETknhhuFBZk1gMAshhuiIiIXILhRmHBFnu3FMMNERGRKzDcKMzeLcUxN0RERK7BcKOwq+GmBEIIlWtDRETkfhhuFGYfc1NUZkV+aYXKtSEiInI/DDcKM+o85RtonrlcrHJtiIiI3A/DjQoi/I0AgIxLRSrXhIiIyP0w3Kgg/Eq4OcVwQ0RE5HQMNypgyw0REZHrMNyogOGGiIjIdRhuVGAPN+kXGW6IiIicjeFGBc0DTAAqW27KKmwq14aIiMi9MNyoIMish4/BE1abwG8XCtSuDhERkVthuFGBJEloFeQDAEjNyle5NkRERO6F4UYlLYMZboiIiFyB4UYlrRluiIiIXILhRiWtg80AgMNn81SuCRERkXthuFFJu1AzNBKQlVeCrNwStatDRETkNhhuVOKt95Rbb3anX1K5NkRERO6D4UZF3Zr5AwB2/nZR5ZoQERG5D4YbFfVoXhluth+/CCGEyrUhIiJyDww3KurZojF0Hhr8dqEQx85xMT8iIiJnYLhRkdmgRd+WjQEA3x3IVLk2RERE7oHhRmWD24cAAFYfZLghIiJyBoYblQ1sGwSth4Rj5wpwLJsL+hEREd0qhhuVWby06BsdAABYtOuUyrUhIiJq+Bhu6oHRPSIAAF//nIHconKVa0NERNSwMdzUA/1bBqJFoAmFZVZ8tOWE2tUhIiJq0Bhu6gGNRsLku1sCAD748TfsO5WjboWIiIgaMIabeiK+XTDu7RACq03g/xbtQ1FZhdpVIiIiapAYbuoJSZLwamIMgsx6pF0oRNuX13LVYiIiojpQNdxs3rwZQ4cORWhoKCRJwooVK2otn5KSAkmSqjyysrKUqbCLWYxavDWik/y85YtrUFxmVa9CREREDZCq4aawsBAdO3bE+++/f1P7paamIjMzU34EBga6qIbK69miMYbfEQYAKLcKtHn5eyzZzSniREREN8pTzTcfPHgwBg8efNP7BQYGwtfX1/kVqifeHNEJf2gdiEkL9gIAnlt6AFuPX8C43s3QoYmvupUjIiKq51QNN3XVqVMnlJaWon379pg+fTp69epVY9nS0lKUlpbKz/Py8pSo4i27t0MoIv29MfS9rQCAlfvOYuW+swAAb50H1v5fXzTxM6pZRSIionqpQQ0oDgkJwQcffIBvvvkG33zzDcLDw9G/f3/88ssvNe6TnJwMi8UiP8LDwxWs8a2JaWLByVlD8N9JvTGsc5i8vbDMit6vbULTv32HZlO/w4KfMlBSzrE5REREACCJejIlR5IkLF++HImJiTe1X79+/RAREYEvvvii2tera7kJDw9Hbm4uzGbzrVRZcacvF6H3a5vgrfNAYQ0DjUd3j0B+SQX+fk8bBFsMCteQiIjINfLy8mCxWG7o+7tBdktdq1u3bti6dWuNr+v1euj1egVr5DpN/Iw4OWsIACC3qBwdZ/5QpcxXP2UAAL7dX9mF1SbEjIe6R6BPi8aIbGSEJEnKVZiIiEgFDT7c7Nu3DyEhIWpXQ3EWo1YOOkIIfH8oC/tO56CwtAJf7syQyx3NzMNLKw5V7uOlRW5x5b2rXrs/Bk38jAjz9ULTxt7KnwAREZGLqBpuCgoKcPz4cfl5Wloa9u3bB39/f0RERGDq1Kk4c+YMPv/8cwDA22+/jWbNmqFdu3YoKSnBxx9/jI0bN+KHH6q2YNxOJEnC4JgQDI6pDHn/SIzBxYJSrDuSjbWHs7Ap9Tx0nho52ADAlG8O1ni8TuG+iG8XjFHdwmHx0rK1h4iIGhRVw83u3btx5513ys//+te/AgCSkpIwf/58ZGZmIiPjaitEWVkZnn32WZw5cwZGoxEdOnTA+vXrHY5BlRqZ9HiwWwQe7FZ5x/GyChv+l52PMZ/+jEuFZYiN9MOe9MvV7rvvVA72ncrBa9//6rA9oVMogsyGKw89Gnnr0cikg1HnAZsNCLYYoPNsUGPUiYjIDdWbAcVKuZkBSe5OCIHTl4vx24VCJH36s0veo7FJj9ziMjTxMyIqwBvrj56rUqZ3i8bYevwCAKBbM3/8nHYJADCscxguFZbhtwsFCPQxIMzXC0FmPbLyShHm6wWLlxYAcLGgFBt+PYe0C4V44Z428DVqoZEkaDRAabkNWXkl8NRIeGv9MTzWpzkOnM5B16b+aB7gDY0kITuvBJ9uTcOzd7fCbxcKEBNmQV5JBY6czcPZnGIkdg6Dj8ETBq0HcovKsfpgJi4XlaFlkA/0nhq0C7Mgt6gcZVYbthw7j7vaBsNs8MR/D2QiwKSHp4cECYDZS4vWwT6w2gT8vHXQSMCOExcR08QXWo0EjUbCwdO5CDTrofXQ4GxOMVoEmpBXUgFPjYSffruIuKhGOJNTAq2HhNJyG87kFMNs8IQAYNB6IKeoDJIkIcCkR/MAb/h76yqvNQAhgPP5pdhy7DwGtQ+GUeeJgtIKFJdZrzv4/Hp/Jqw2gYLSCpj0njibW4Izl4tRYbXhclE50i4UoFeLxjBoPfBL+mUkdg6Dv7cOVpuATQiUVthwLr8Uek8NNh49h+ggEzw1Ei4WluGutkE4m1OC3OIy5BVXIMhigBACHhoJwWaDfG5X61n533KrDScvFuLAqVwUl1th9vJEY5MejUx6tAs149fMfKRfLIRJ74keUY1wqbAMEoDCsgr4e+tRXGbF5aIyGHUeyMwtQYjFgKgAEySp8j3WHclGrxaNYbUJ/JR2EY1MepRV2FBhtQEAjp8rQNtQM8oqbMgtLkdxuRUdm/gir6QcWbkl8PfWoajMipZBJpzLL4WX1gMeGglnLhcjKtAEg1aD384Xom2oGUadJ4QQ2PHbRYT5euFMTjF8vXRoHeJz5XwFsvNKkV9SDq2HBsEWA/SeGqRmFUBA4OSFQlwsLEN0oA/ySsqRcbEIg2OCEelftTu6uNyKzf87j7O5xXggNhz7TuXA7OWJn367hNhIPxSUVkDvqYHFSwtvvScOnslFEz8vSJAQZNZjwU8ZiGhkhEnvibYhZoT4esFqs13zDo6twNc2CksO26Uq209fLsbrP6SiZ1QjBPjocaGgFHHNG2P7iQsYfkcTaK4UTLtQiF0nL+HOVoEwGTzhpfXA1mMX4G/SYU/6ZdhsAu3DLGji54UzOSWICvCGSe8JjysHqO697XUVAthwNBsFpVaE+3sh/WIRfAyeyC+pQKBZDyGAqADvK58ZL+SXlMOo80SwpfI1+++hEAJ5JRXYcuw8IvyNCLYY0LSRN679NRMQsAlgy//Oo0O4L85cLkZjkw6NTDocyy7AD0eycWfrQIRaDPg1Kx8dm/jCZPB0qHN2Xgl+ychBq2AT8ksqr124X+XYy9OXi1BaYUPLIB9IUuW5Xj11CTlFZTDpPZFXUoEAH738feFx5XezwmrDmcvFiI30w7n8UrQPs2DF3jPo2swfYb5eMOo80CbEud+xN/P9zXBD1RJCIDO3BNl5Jfhs+0nENvVHSZkVWXmV287llSIrrwT5JeW4XFR+/QMSEdFtIzbSD99M6OnUY95Ws6XINSRJQqivF0J9vdA5wq/WslabQGmFFWdzilFSbsOhM7n4Ke0S2oaYIUnA5mMXEGzWIybMgn2ncvHNL6flfRub9PDQANl5ldP19Z4alFbY4K3zgCRJKCi9enf0OyJ88UtGDloGmfC/7AJoJGBIh1Dkl5QjJfW8XO7OVgGwCaC4zIqfT15Ct6b+8DF4YsOvV1uNdB4adG3mhwqrwIHTlf93D1TOLjPpPXA+vxQnLxbJdWzkrUNphVXeVp1QiwFnc0tg1HmgtMIGq636/28INhvgpfNA2oVCeVvTRkb52D56T+SXOueu8E38vOT/G5MgIePS1fpHNjIi/WIR/L110Hlo4OlR+9iq2oZenbpUDABo1tgbZRWVLUq1CTYb4KG52rp2Lr+01vJ2gT56h7LNrgyGd6iaBGTllqDoBu/L5mvUIud3Ad3eQvL7OtuEcHh/k97T4TPqCtdOBPh9fXSeGkgSkH7lsyNJlddciMqWjtqEWAz4/SU9m1virGrLddRrNZDg2MIGoEorRbXbr/l3bZ8pT42EYIsBknT1s2jn763DpcKyWusZ5nv196S2/90XQtTpZxTu7wUP+/0Qr2z77ZrffwBobNLBU6ORW1EAIL+0Avkljp+v3/990EiATVT+bmg9HIcFXPszs+/nrfOAxUsrn0eYrxe0V3737acuBOS/FY1NOug9PWC1CWTl3di5RzYyyi2ramHLDREREdV7N/P9zdGfRERE5FYYboiIiMitMNwQERGRW2G4ISIiIrfCcENERERuheGGiIiI3ArDDREREbkVhhsiIiJyKww3RERE5FYYboiIiMitMNwQERGRW2G4ISIiIrfCcENERERuheGGiIiI3Iqn2hVQmhACQOWt04mIiKhhsH9v27/Ha3PbhZv8/HwAQHh4uMo1ISIiopuVn58Pi8VSaxlJ3EgEciM2mw1nz56Fj48PJEly6rHz8vIQHh6OU6dOwWw2O/XY9YG7nx/g/ufI82v43P0ceX4Nn6vOUQiB/Px8hIaGQqOpfVTNbddyo9Fo0KRJE5e+h9lsdtsPLeD+5we4/zny/Bo+dz9Hnl/D54pzvF6LjR0HFBMREZFbYbghIiIit8Jw40R6vR7Tpk2DXq9Xuyou4e7nB7j/OfL8Gj53P0eeX8NXH87xthtQTERERO6NLTdERETkVhhuiIiIyK0w3BAREZFbYbghIiIit8Jw4yTvv/8+mjZtCoPBgO7du+Pnn39Wu0rVSk5ORteuXeHj44PAwEAkJiYiNTXVoUz//v0hSZLD44knnnAok5GRgSFDhsBoNCIwMBDPPfccKioqHMqkpKTgjjvugF6vR4sWLTB//nxXnx6mT59epe6tW7eWXy8pKcHEiRPRqFEjmEwm3H///cjOzm4Q52bXtGnTKucoSRImTpwIoOFdv82bN2Po0KEIDQ2FJElYsWKFw+tCCLz88ssICQmBl5cXBg4ciGPHjjmUuXTpEkaPHg2z2QxfX1+MGzcOBQUFDmUOHDiAPn36wGAwIDw8HLNnz65SlyVLlqB169YwGAyIiYnB6tWrXXp+5eXlmDJlCmJiYuDt7Y3Q0FCMGTMGZ8+edThGddd81qxZ9eL8rneOADB27Ngq9R80aJBDmYZ6DQFU+/soSRLmzJkjl6nP1/BGvheU/NvplO9TQbds4cKFQqfTiU8//VQcPnxYPPbYY8LX11dkZ2erXbUq4uPjxbx588ShQ4fEvn37xD333CMiIiJEQUGBXKZfv37iscceE5mZmfIjNzdXfr2iokK0b99eDBw4UOzdu1esXr1aNG7cWEydOlUu89tvvwmj0Sj++te/iiNHjoh3331XeHh4iO+//96l5zdt2jTRrl07h7qfP39efv2JJ54Q4eHhYsOGDWL37t2iR48eomfPng3i3OzOnTvncH7r1q0TAMSmTZuEEA3v+q1evVq88MILYtmyZQKAWL58ucPrs2bNEhaLRaxYsULs379f3HfffaJZs2aiuLhYLjNo0CDRsWNHsXPnTrFlyxbRokULMWrUKPn13NxcERQUJEaPHi0OHTokvv76a+Hl5SU+/PBDucy2bduEh4eHmD17tjhy5Ih48cUXhVarFQcPHnTZ+eXk5IiBAweKRYsWiV9//VXs2LFDdOvWTcTGxjocIzIyUsycOdPhml77O6vm+V3vHIUQIikpSQwaNMih/pcuXXIo01CvoRDC4bwyMzPFp59+KiRJEidOnJDL1OdreCPfC0r97XTW9ynDjRN069ZNTJw4UX5utVpFaGioSE5OVrFWN+bcuXMCgPjxxx/lbf369RNPP/10jfusXr1aaDQakZWVJW+bO3euMJvNorS0VAghxPPPPy/atWvnsN/IkSNFfHy8c0/gd6ZNmyY6duxY7Ws5OTlCq9WKJUuWyNuOHj0qAIgdO3YIIer3udXk6aefFlFRUcJmswkhGvb1+/0Xh81mE8HBwWLOnDnytpycHKHX68XXX38thBDiyJEjAoDYtWuXXGbNmjVCkiRx5swZIYQQ//73v4Wfn598fkIIMWXKFNGqVSv5+YgRI8SQIUMc6tO9e3fx+OOPu+z8qvPzzz8LACI9PV3eFhkZKd56660a96kv5ydE9eeYlJQkEhISatzH3a5hQkKC+MMf/uCwrSFdw99/Lyj5t9NZ36fslrpFZWVl2LNnDwYOHChv02g0GDhwIHbs2KFizW5Mbm4uAMDf399h+1dffYXGjRujffv2mDp1KoqKiuTXduzYgZiYGAQFBcnb4uPjkZeXh8OHD8tlrv2Z2Mso8TM5duwYQkND0bx5c4wePRoZGRkAgD179qC8vNyhXq1bt0ZERIRcr/p+br9XVlaGL7/8Eo888ojDjWAb8vW7VlpaGrKyshzqYrFY0L17d4dr5uvriy5dushlBg4cCI1Gg59++kku07dvX+h0OrlMfHw8UlNTcfnyZblMfTjn3NxcSJIEX19fh+2zZs1Co0aN0LlzZ8yZM8ehub8hnF9KSgoCAwPRqlUrTJgwARcvXnSov7tcw+zsbHz33XcYN25cldcayjX8/feCUn87nfl9etvdONPZLly4AKvV6nBBASAoKAi//vqrSrW6MTabDc888wx69eqF9u3by9sfeughREZGIjQ0FAcOHMCUKVOQmpqKZcuWAQCysrKqPV/7a7WVycvLQ3FxMby8vFxyTt27d8f8+fPRqlUrZGZmYsaMGejTpw8OHTqErKws6HS6Kl8aQUFB1613fTi36qxYsQI5OTkYO3asvK0hX7/fs9enurpcW9fAwECH1z09PeHv7+9QplmzZlWOYX/Nz8+vxnO2H0MJJSUlmDJlCkaNGuVww8GnnnoKd9xxB/z9/bF9+3ZMnToVmZmZePPNN+VzqM/nN2jQIAwfPhzNmjXDiRMn8Pe//x2DBw/Gjh074OHh4VbX8LPPPoOPjw+GDx/usL2hXMPqvheU+tt5+fJlp32fMtzcxiZOnIhDhw5h69atDtvHjx8v/zsmJgYhISEYMGAATpw4gaioKKWreVMGDx4s/7tDhw7o3r07IiMjsXjxYkVDh1I++eQTDB48GKGhofK2hnz9bmfl5eUYMWIEhBCYO3euw2t//etf5X936NABOp0Ojz/+OJKTkxvEMv4PPvig/O+YmBh06NABUVFRSElJwYABA1SsmfN9+umnGD16NAwGg8P2hnINa/peaGjYLXWLGjduDA8PjyqjxrOzsxEcHKxSra5v0qRJWLVqFTZt2oQmTZrUWrZ79+4AgOPHjwMAgoODqz1f+2u1lTGbzYqGDF9fX7Rs2RLHjx9HcHAwysrKkJOTU6Ve16u3/bXayih9bunp6Vi/fj0effTRWss15Otnr09tv1/BwcE4d+6cw+sVFRW4dOmSU66rEr/H9mCTnp6OdevWObTaVKd79+6oqKjAyZMnAdT/8/u95s2bo3Hjxg6fyYZ+DQFgy5YtSE1Nve7vJFA/r2FN3wtK/e105vcpw80t0ul0iI2NxYYNG+RtNpsNGzZsQFxcnIo1q54QApMmTcLy5cuxcePGKs2g1dm3bx8AICQkBAAQFxeHgwcPOvwxsv9Bbtu2rVzm2p+JvYzSP5OCggKcOHECISEhiI2NhVardahXamoqMjIy5Ho1pHObN28eAgMDMWTIkFrLNeTr16xZMwQHBzvUJS8vDz/99JPDNcvJycGePXvkMhs3boTNZpODXVxcHDZv3ozy8nK5zLp169CqVSv4+fnJZdQ4Z3uwOXbsGNavX49GjRpdd599+/ZBo9HIXTn1+fyqc/r0aVy8eNHhM9mQr6HdJ598gtjYWHTs2PG6ZevTNbze94JSfzud+n16U8OPqVoLFy4Uer1ezJ8/Xxw5ckSMHz9e+Pr6Oowary8mTJggLBaLSElJcZiSWFRUJIQQ4vjx42LmzJli9+7dIi0tTaxcuVI0b95c9O3bVz6Gfcrf3XffLfbt2ye+//57ERAQUO2Uv+eee04cPXpUvP/++4pMl3722WdFSkqKSEtLE9u2bRMDBw4UjRs3FufOnRNCVE5njIiIEBs3bhS7d+8WcXFxIi4urkGc27WsVquIiIgQU6ZMcdjeEK9ffn6+2Lt3r9i7d68AIN58802xd+9eebbQrFmzhK+vr1i5cqU4cOCASEhIqHYqeOfOncVPP/0ktm7dKqKjox2mEefk5IigoCDx5z//WRw6dEgsXLhQGI3GKtNsPT09xeuvvy6OHj0qpk2b5pRptrWdX1lZmbjvvvtEkyZNxL59+xx+J+0zTLZv3y7eeustsW/fPnHixAnx5ZdfioCAADFmzJh6cX7XO8f8/HwxefJksWPHDpGWlibWr18v7rjjDhEdHS1KSkrkYzTUa2iXm5srjEajmDt3bpX96/s1vN73ghDK/e101vcpw42TvPvuuyIiIkLodDrRrVs3sXPnTrWrVC0A1T7mzZsnhBAiIyND9O3bV/j7+wu9Xi9atGghnnvuOYd1UoQQ4uTJk2Lw4MHCy8tLNG7cWDz77LOivLzcocymTZtEp06dhE6nE82bN5ffw5VGjhwpQkJChE6nE2FhYWLkyJHi+PHj8uvFxcXiySefFH5+fsJoNIphw4aJzMzMBnFu11q7dq0AIFJTUx22N8Trt2nTpmo/k0lJSUKIyungL730kggKChJ6vV4MGDCgynlfvHhRjBo1SphMJmE2m8XDDz8s8vPzHcrs379f9O7dW+j1ehEWFiZmzZpVpS6LFy8WLVu2FDqdTrRr10589913Lj2/tLS0Gn8n7esW7dmzR3Tv3l1YLBZhMBhEmzZtxD//+U+HYKDm+V3vHIuKisTdd98tAgIChFarFZGRkeKxxx6r8mXVUK+h3Ycffii8vLxETk5Olf3r+zW83veCEMr+7XTG96l05cSIiIiI3ALH3BAREZFbYbghIiIit8JwQ0RERG6F4YaIiIjcCsMNERERuRWGGyIiInIrDDdERETkVhhuiMjlmjZtirfffvuGy6ekpECSpCr3siEiuhEMN0QkkySp1sf06dPrdNxdu3Y53K38enr27InMzExYLJY6vd/N+M9//oOOHTvCZDLB19cXnTt3RnJysvz62LFjkZiY6PJ6EJHzeKpdASKqPzIzM+V/L1q0CC+//DJSU1PlbSaTSf63EAJWqxWentf/MxIQEHBT9dDpdIrcyfnTTz/FM888g3/961/o168fSktLceDAARw6dMjl701ErsOWGyKSBQcHyw+LxQJJkuTnv/76K3x8fLBmzRrExsZCr9dj69atOHHiBBISEhAUFASTyYSuXbti/fr1Dsf9fbeUJEn4+OOPMWzYMBiNRkRHR+Pbb7+VX/99t9T8+fPh6+uLtWvXok2bNjCZTBg0aJBDGKuoqMBTTz0FX19fNGrUCFOmTEFSUlKtrS7ffvstRowYgXHjxqFFixZo164dRo0ahVdffRUAMH36dHz22WdYuXKl3HqVkpICADh16hRGjBgBX19f+Pv7IyEhASdPnpSPbW/xmTFjBgICAmA2m/HEE0+grKxMLrN06VLExMTAy8sLjRo1wsCBA1FYWHiTV42Ifo/hhohuyt/+9jfMmjULR48eRYcOHVBQUIB77rkHGzZswN69ezFo0CAMHToUGRkZtR5nxowZGDFiBA4cOIB77rkHo0ePxqVLl2osX1RUhNdffx1ffPEFNm/ejIyMDEyePFl+/bXXXsNXX32FefPmYdu2bcjLy8OKFStqrUNwcDB27tyJ9PT0al+fPHkyRowYIQepzMxM9OzZE+Xl5YiPj4ePjw+2bNmCbdu2yYHr2vCyYcMGHD16FCkpKfj666+xbNkyzJgxA0BlK9moUaPwyCOPyGWGDx8O3u6PyAlu+labRHRbmDdvnrBYLPJz+52RV6xYcd1927VrJ9599135eWRkpHjrrbfk5wDEiy++KD8vKCgQAMSaNWsc3uvy5ctyXQA43OH9/fffF0FBQfLzoKAgMWfOHPl5RUWFiIiIEAkJCTXW8+zZs6JHjx4CgGjZsqVISkoSixYtElarVS6TlJRU5RhffPGFaNWqlbDZbPK20tJS4eXlJdauXSvv5+/vLwoLC+Uyc+fOFSaTSVitVrFnzx4BQJw8ebLG+hFR3bDlhohuSpcuXRyeFxQUYPLkyWjTpg18fX1hMplw9OjR67bcdOjQQf63t7c3zGYzzp07V2N5o9GIqKgo+XlISIhcPjc3F9nZ2ejWrZv8uoeHB2JjY2utQ0hICHbs2IGDBw/i6aefRkVFBZKSkjBo0CDYbLYa99u/fz+OHz8OHx8fmEwmmEwm+Pv7o6SkBCdOnJDLdezYEUajUX4eFxeHgoICnDp1Ch07dsSAAQMQExODBx54AP/5z39w+fLlWutLRDeGA4qJ6KZ4e3s7PJ88eTLWrVuH119/HS1atICXlxf++Mc/OnTPVEer1To8lySp1kBRXXnhpC6c9u3bo3379njyySfxxBNPoE+fPvjxxx9x5513Vlu+oKAAsbGx+Oqrr6q8dqODpz08PLBu3Tps374dP/zwA95991288MIL+Omnn9CsWbNbOh+i2x1bbojolmzbtg1jx47FsGHDEBMTg+DgYIeBtUqwWCwICgrCrl275G1WqxW//PLLTR+rbdu2ACAP7NXpdLBarQ5l7rjjDhw7dgyBgYFo0aKFw+Pa6ev79+9HcXGx/Hznzp0wmUwIDw8HUBnQevXqhRkzZmDv3r3Q6XRYvnz5TdeZiBwx3BDRLYmOjsayZcuwb98+7N+/Hw899FCtLTCu8pe//AXJyclYuXIlUlNT8fTTT+Py5cuQJKnGfSZMmIBXXnkF27ZtQ3p6Onbu3IkxY8YgICAAcXFxACpneh04cACpqam4cOECysvLMXr0aDRu3BgJCQnYsmUL0tLSkJKSgqeeegqnT5+Wj19WVoZx48bhyJEjWL16NaZNm4ZJkyZBo9Hgp59+wj//+U/s3r0bGRkZWLZsGc6fP482bdq4/GdF5O4Ybojolrz55pvw8/NDz549MXToUMTHx+OOO+5QvB5TpkzBqFGjMGbMGMTFxcFkMiE+Ph4Gg6HGfQYOHIidO3figQceQMuWLXH//ffDYDBgw4YNaNSoEQDgscceQ6tWrdClSxcEBARg27ZtMBqN2Lx5MyIiIjB8+HC0adMG48aNQ0lJCcxms3z8AQMGIDo6Gn379sXIkSNx3333yQshms1mbN68Gffccw9atmyJF198EW+88QYGDx7s0p8T0e1AEs7qtCYiqkdsNhvatGmDESNG4JVXXlH8/ceOHYucnJzrTkcnIufjgGIicgvp6en44Ycf5JWG33vvPaSlpeGhhx5Su2pEpDB2SxGRW9BoNJg/fz66du2KXr164eDBg1i/fj3HsBDdhtgtRURERG6FLTdERETkVhhuiIiIyK0w3BAREZFbYbghIiIit8JwQ0RERG6F4YaIiIjcCsMNERERuRWGGyIiInIrDDdERETkVv4fuaGUU5uCM5EAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "samples = jax.random.normal(subkey, (n_samples,dim))\n",
    "opt = optax.adam(1e-3)\n",
    "opt_state = opt.init(samples)\n",
    "print(\"Sample shape\", samples.shape)\n",
    "\n",
    "# Training loop\n",
    "@jax.jit\n",
    "def train_step(samples, opt_state):\n",
    "    value_loss, grads1 = jax.value_and_grad(loss_fn_entropy_values)(samples, means, stds_diag, true_entropies)\n",
    "    gradient_loss, grads2 = jax.value_and_grad(loss_fn_entropy_gradients)(samples, means, stds_diag, true_entropy_grads_mu, true_entropy_grads_mu)\n",
    "    grads = grads1 + 0*grads2  # Combine gradients for both losses\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    samples = optax.apply_updates(samples, updates)\n",
    "    return samples, opt_state, value_loss, gradient_loss \n",
    "\n",
    "value_losses = []\n",
    "gradient_losses = []\n",
    "for i in tqdm.trange(20_000):\n",
    "    samples, opt_state, loss1, loss2 = train_step(samples, opt_state)\n",
    "    value_losses.append(loss1)\n",
    "    gradient_losses.append(loss2)\n",
    "\n",
    "print(\"Final learned samples (z):\", (samples).sort())\n",
    "plt.plot(value_losses, label='Value Loss')\n",
    "# plt.plot(gradient_losses, label='Gradient Loss')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Steps')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ac07d",
   "metadata": {},
   "source": [
    "### Learned Samples for fast MC entropy estimation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1bc697ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half set [-1.2608293294906616, -1.2133342027664185, -0.4514565169811249, 0.10739301145076752, 0.10739489644765854, 0.5797147750854492, 0.579728901386261, 1.8561625480651855]\n",
      "Full set [-1.8561625480651855, -1.2608293294906616, -1.2133342027664185, -0.579728901386261, -0.5797147750854492, -0.4514565169811249, -0.10739489644765854, -0.10739301145076752, 0.10739301145076752, 0.10739489644765854, 0.4514565169811249, 0.5797147750854492, 0.579728901386261, 1.2133342027664185, 1.2608293294906616, 1.8561625480651855]\n"
     ]
    }
   ],
   "source": [
    "if dim == 1:\n",
    "    samples = samples.squeeze()\n",
    "    print('Half set', [i.item() for i in samples.sort()])\n",
    "    ## Mirror the smaples to make it symmetric around zero\n",
    "    samples = jnp.concatenate([-samples[::-1], samples])  # Adding zero to ensure symmetry\n",
    "    print('Full set', [i.item() for i in samples.sort()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b00d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "[-2.28250789642334, -1.9211180210113525, -1.3525558710098267, -1.0677014589309692, -0.8902221918106079, -0.8044639229774475, -0.7819824814796448, -0.7775918245315552, -0.777317225933075, -0.7008596658706665, -0.6292149424552917, -0.29937660694122314, -0.1358855664730072, -0.09546408802270889, -0.03650583326816559, -0.012498264200985432, 0.012498264200985432, 0.03650583326816559, 0.09546408802270889, 0.1358855664730072, 0.29937660694122314, 0.6292149424552917, 0.7008596658706665, 0.777317225933075, 0.7775918245315552, 0.7819824814796448, 0.8044639229774475, 0.8902221918106079, 1.0677014589309692, 1.3525558710098267, 1.9211180210113525, 2.28250789642334]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed12c0",
   "metadata": {},
   "source": [
    "# Optimal samples for entorpy gradient estimation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f266720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, random\n",
    "import optax\n",
    "\n",
    "# ------------------------------------------\n",
    "# Tanh Gaussian Entropy using Monte Carlo\n",
    "# ------------------------------------------\n",
    "def tanh_gaussian_entropy_mc(samples, mu, log_std):\n",
    "    std = jnp.exp(log_std)\n",
    "    z = mu + samples * std\n",
    "    y = jnp.tanh(z)\n",
    "    log_det_jacobian = jnp.sum(jnp.log(1 - jnp.square(y) + 1e-6), axis=-1)\n",
    "    base_log_prob = -0.5 * jnp.sum(jnp.square(samples) + 2 * log_std + jnp.log(2 * jnp.pi), axis=-1)\n",
    "    return -jnp.mean(base_log_prob - log_det_jacobian)\n",
    "\n",
    "# ------------------------------------------\n",
    "# True Entropy Gradient with Large Sample Set\n",
    "# ------------------------------------------\n",
    "def true_entropy_grad(mu, log_std, key, num_samples=10000):\n",
    "    dim = mu.shape[0]\n",
    "    samples = random.normal(key, shape=(num_samples, dim))\n",
    "    entropy_fn = lambda m, l: tanh_gaussian_entropy_mc(samples, m, l)\n",
    "    grad_mu = grad(entropy_fn, argnums=0)(mu, log_std)\n",
    "    grad_log_std = grad(entropy_fn, argnums=1)(mu, log_std)\n",
    "    return grad_mu, grad_log_std\n",
    "\n",
    "# ------------------------------------------\n",
    "# Estimated Entropy Gradient with Learned Samples\n",
    "# ------------------------------------------\n",
    "def estimated_entropy_grad(samples_learned, mu, log_std):\n",
    "    entropy_fn = lambda m, l: tanh_gaussian_entropy_mc(samples_learned, m, l)\n",
    "    grad_mu = grad(entropy_fn, argnums=0)(mu, log_std)\n",
    "    grad_log_std = grad(entropy_fn, argnums=1)(mu, log_std)\n",
    "    return grad_mu, grad_log_std\n",
    "\n",
    "# ------------------------------------------\n",
    "# Gradient Matching Loss Function\n",
    "# ------------------------------------------\n",
    "def gradient_matching_loss(samples_learned, mu, log_std, key):\n",
    "    true_grad_mu, true_grad_log_std = true_entropy_grad(mu, log_std, key)\n",
    "    est_grad_mu, est_grad_log_std = estimated_entropy_grad(samples_learned, mu, log_std)\n",
    "    loss = jnp.sum((true_grad_mu - est_grad_mu) ** 2) + jnp.sum((true_grad_log_std - est_grad_log_std) ** 2)\n",
    "    return loss\n",
    "\n",
    "# ------------------------------------------\n",
    "# Optimization Loop\n",
    "# ------------------------------------------\n",
    "def optimize_entropy_samples(dim=3, n_learned_samples=16, n_iters=1000, lr=1e-2, seed=0):\n",
    "    key = random.PRNGKey(seed)\n",
    "    mu = jnp.zeros((dim,))\n",
    "    log_std = jnp.zeros((dim,))\n",
    "    key, subkey = random.split(key)\n",
    "    samples_learned = random.normal(subkey, shape=(n_learned_samples, dim))\n",
    "\n",
    "    optimizer = optax.adam(lr)\n",
    "    opt_state = optimizer.init(samples_learned)\n",
    "\n",
    "    @jax.jit\n",
    "    def step(samples, opt_state, key):\n",
    "        loss, grads = jax.value_and_grad(gradient_matching_loss)(samples, mu, log_std, key)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, samples)\n",
    "        samples = optax.apply_updates(samples, updates)\n",
    "        return samples, opt_state, loss\n",
    "\n",
    "    losses = []\n",
    "    for i in range(n_iters):\n",
    "        key, subkey = random.split(key)\n",
    "        samples_learned, opt_state, loss = step(samples_learned, opt_state, subkey)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return samples_learned, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78fecb7",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46068251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, x: 0.8744005687622104\n",
      "Iteration 101, x: 0.8744477486715041\n",
      "Iteration 201, x: 0.8744799446041801\n",
      "Iteration 301, x: 0.8745019159428646\n",
      "Iteration 401, x: 0.8745169100269383\n",
      "Iteration 501, x: 0.8745271426921587\n",
      "Iteration 601, x: 0.8745341260014181\n",
      "Iteration 701, x: 0.8745388918067687\n",
      "Iteration 801, x: 0.8745421442748413\n",
      "Iteration 901, x: 0.8745443639578373\n",
      "Iteration 1001, x: 0.8745458788079644\n",
      "Iteration 1101, x: 0.8745469126374767\n",
      "Iteration 1201, x: 0.874547618188712\n",
      "Iteration 1301, x: 0.874548099702182\n",
      "Iteration 1401, x: 0.8745484283180266\n",
      "Iteration 1501, x: 0.8745486525867312\n",
      "Iteration 1601, x: 0.8745488056422283\n",
      "Iteration 1701, x: 0.8745489100972306\n",
      "Iteration 1801, x: 0.8745489813841085\n",
      "Iteration 1901, x: 0.8745490300349074\n",
      "Iteration 2001, x: 0.8745490632373749\n",
      "Iteration 2101, x: 0.8745490858968986\n",
      "Iteration 2201, x: 0.8745491013612311\n",
      "Iteration 2301, x: 0.8745491119150979\n",
      "Iteration 2401, x: 0.8745491191177429\n",
      "Iteration 2501, x: 0.8745491240332965\n",
      "Iteration 2601, x: 0.8745491273879901\n",
      "Iteration 2701, x: 0.8745491296774514\n",
      "Iteration 2801, x: 0.874549131239929\n",
      "Iteration 2901, x: 0.8745491323062652\n",
      "Iteration 3001, x: 0.8745491330340021\n",
      "Iteration 3101, x: 0.874549133530657\n",
      "Iteration 3201, x: 0.8745491338696069\n",
      "Iteration 3301, x: 0.8745491341009284\n",
      "Iteration 3401, x: 0.8745491342587978\n",
      "Iteration 3501, x: 0.8745491343665377\n",
      "Iteration 3601, x: 0.8745491344400667\n",
      "Iteration 3701, x: 0.8745491344902471\n",
      "Iteration 3801, x: 0.8745491345244937\n",
      "Iteration 3901, x: 0.8745491345478658\n",
      "Iteration 4001, x: 0.874549134563816\n",
      "Iteration 4101, x: 0.874549134574702\n",
      "Iteration 4201, x: 0.8745491345821311\n",
      "Iteration 4301, x: 0.8745491345872012\n",
      "Iteration 4401, x: 0.8745491345906609\n",
      "Iteration 4501, x: 0.8745491345930227\n",
      "Iteration 4601, x: 0.8745491345946342\n",
      "Iteration 4701, x: 0.8745491345957344\n",
      "Iteration 4801, x: 0.8745491345964851\n",
      "Iteration 4901, x: 0.8745491345969975\n",
      "Iteration 5001, x: 0.8745491345973472\n",
      "Iteration 5101, x: 0.8745491345975859\n",
      "Iteration 5201, x: 0.8745491345977484\n",
      "Iteration 5301, x: 0.8745491345978595\n",
      "Iteration 5401, x: 0.8745491345979356\n",
      "Iteration 5501, x: 0.8745491345979869\n",
      "Iteration 5601, x: 0.8745491345980226\n",
      "Iteration 5701, x: 0.874549134598046\n",
      "Iteration 5801, x: 0.8745491345980616\n",
      "Iteration 5901, x: 0.8745491345980727\n",
      "Iteration 6001, x: 0.8745491345980838\n",
      "Iteration 6101, x: 0.874549134598084\n",
      "Iteration 6201, x: 0.874549134598084\n",
      "Iteration 6301, x: 0.874549134598084\n",
      "Iteration 6401, x: 0.874549134598084\n",
      "Iteration 6501, x: 0.874549134598084\n",
      "Iteration 6601, x: 0.874549134598084\n",
      "Iteration 6701, x: 0.874549134598084\n",
      "Iteration 6801, x: 0.874549134598084\n",
      "Iteration 6901, x: 0.874549134598084\n",
      "Iteration 7001, x: 0.874549134598084\n",
      "Iteration 7101, x: 0.874549134598084\n",
      "Iteration 7201, x: 0.874549134598084\n",
      "Iteration 7301, x: 0.874549134598084\n",
      "Iteration 7401, x: 0.874549134598084\n",
      "Iteration 7501, x: 0.874549134598084\n",
      "Iteration 7601, x: 0.874549134598084\n",
      "Iteration 7701, x: 0.874549134598084\n",
      "Iteration 7801, x: 0.874549134598084\n",
      "Iteration 7901, x: 0.874549134598084\n",
      "Iteration 8001, x: 0.874549134598084\n",
      "Iteration 8101, x: 0.874549134598084\n",
      "Iteration 8201, x: 0.874549134598084\n",
      "Iteration 8301, x: 0.874549134598084\n",
      "Iteration 8401, x: 0.874549134598084\n",
      "Iteration 8501, x: 0.874549134598084\n",
      "Iteration 8601, x: 0.874549134598084\n",
      "Iteration 8701, x: 0.874549134598084\n",
      "Iteration 8801, x: 0.874549134598084\n",
      "Iteration 8901, x: 0.874549134598084\n",
      "Iteration 9001, x: 0.874549134598084\n",
      "Iteration 9101, x: 0.874549134598084\n",
      "Iteration 9201, x: 0.874549134598084\n",
      "Iteration 9301, x: 0.874549134598084\n",
      "Iteration 9401, x: 0.874549134598084\n",
      "Iteration 9501, x: 0.874549134598084\n",
      "Iteration 9601, x: 0.874549134598084\n",
      "Iteration 9701, x: 0.874549134598084\n",
      "Iteration 9801, x: 0.874549134598084\n",
      "Iteration 9901, x: 0.874549134598084\n",
      "Optimized x: 0.874549134598084\n",
      "Entropy value: 0.6837350076936062\n",
      "Gradient value: 2.7755575615628914e-14\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "key = jax.random.PRNGKey(7)\n",
    "samples = jax.random.normal(key, shape=(1_000_000,), dtype=jnp.float64)  # Example learned samples\n",
    "samples = jnp.concatenate([-samples[::-1], samples, jnp.array([0])])  # Adding zero to ensure symmetry\n",
    "samples = (samples - jnp.mean(samples)) / jnp.std(samples)  # Normalize samples\n",
    "\n",
    "def f(x):\n",
    "    return jnp.mean(jnp.log1p(-jnp.tanh(x * samples) ** 2)) + 0.5 * jnp.log(2 *jnp.pi *jnp.e * x**2)\n",
    "def f_grad(x):\n",
    "    return (1/x) + jnp.mean(-2 * jnp.tanh(x * samples) * samples)\n",
    "\n",
    "@jax.jit\n",
    "def update_f(x):\n",
    "    x += 0.002*f_grad(x)\n",
    "    return x\n",
    "\n",
    "x_0 = 0.8744  # Initial value for x, very close to the optimal value\n",
    "x_0 = jnp.array(x_0, dtype=jnp.float64)  # Ensure x_0 is a float64 for higher precision\n",
    "\n",
    "for i in range(10_000):\n",
    "    x_0 = update_f(x_0)\n",
    "    if i%100==0:\n",
    "        print(f\"Iteration {i+1}, x: {x_0}\")\n",
    "print(\"Optimized x:\", x_0)\n",
    "print(\"Entropy value:\", f(x_0))\n",
    "print(\"Gradient value:\", f_grad(x_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb2e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, x: 0.8744000196456909\n",
      "Iteration 101, x: 0.8744000196456909\n",
      "Iteration 201, x: 0.8744000196456909\n",
      "Iteration 301, x: 0.8744000196456909\n",
      "Iteration 401, x: 0.8744000196456909\n",
      "Iteration 501, x: 0.8744000196456909\n",
      "Iteration 601, x: 0.8744000196456909\n",
      "Iteration 701, x: 0.8744000196456909\n",
      "Iteration 801, x: 0.8744000196456909\n",
      "Iteration 901, x: 0.8744000196456909\n",
      "Optimized x: 0.8744\n",
      "Entropy value: 0.6836473\n",
      "Gradient value: 2.2053719e-05\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2906781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8744062185287476"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8727344274520874 # 100\n",
    "0.8727468252182007 # 100k\n",
    "0.8743883967399597 # 10M\n",
    "0.8744339942932129 # 100M\n",
    "0.874382734298706  # 100M\n",
    "0.8744062185287476 # 100M\n",
    "0.8744085431098938 # 100M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3027062a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:f32[]\u001b[39m b\u001b[35m:f32[]\u001b[39m c\u001b[35m:f32[]\u001b[39m. \u001b[34;1mlet\n",
      "    \u001b[39;22md\u001b[35m:f32[]\u001b[39m = mul a b\n",
      "    e\u001b[35m:f32[]\u001b[39m = mul d b\n",
      "    f\u001b[35m:f32[]\u001b[39m = mul c b\n",
      "    g\u001b[35m:f32[]\u001b[39m = add e f\n",
      "  \u001b[34;1min \u001b[39;22m(g,) }\n",
      "module @jit_f attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
      "  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<f32>, %arg2: tensor<f32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n",
      "    %0 = stablehlo.multiply %arg0, %arg1 : tensor<f32>\n",
      "    %1 = stablehlo.multiply %0, %arg1 : tensor<f32>\n",
      "    %2 = stablehlo.multiply %arg2, %arg1 : tensor<f32>\n",
      "    %3 = stablehlo.add %1, %2 : tensor<f32>\n",
      "    return %3 : tensor<f32>\n",
      "  }\n",
      "}\n",
      "\n",
      "CompiledMemoryStats(generated_code_size_in_bytes=2984, argument_size_in_bytes=12, output_size_in_bytes=4, alias_size_in_bytes=0, temp_size_in_bytes=0, host_generated_code_size_in_bytes=0, host_argument_size_in_bytes=0, host_output_size_in_bytes=0, host_alias_size_in_bytes=0, host_temp_size_in_bytes=0)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def f(a, x, b):\n",
    "    return a*x*x + b*x\n",
    "\n",
    "# JIT compile\n",
    "f_jit = jax.jit(f, inline=True)\n",
    "\n",
    "# Lower without running\n",
    "# lowered = f_jit.lower(3.0, 2.0, 4.0)\n",
    "traced = f_jit.trace(3.0, 2.0, 4.0)\n",
    "lowered = traced.lower()\n",
    "\n",
    "print(traced.jaxpr)\n",
    "print(lowered.as_text())   # human-readable HLO\n",
    "\n",
    "compiled = lowered.compile()\n",
    "print(compiled.memory_analysis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cfe157a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example(a=1, b=[2, 1])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import jax\n",
    "\n",
    "@jax.tree_util.register_dataclass\n",
    "@dataclass\n",
    "class Example:\n",
    "    a: int\n",
    "    b: list = field(default_factory=list)  # temporary default\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # You can now use self.a to adjust b\n",
    "        self.b = [self.a * 2, 1]\n",
    "\n",
    "params = Example(1)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5fa07edd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "EnvParams.__init__() got an unexpected keyword argument 'num_agents'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m     num_agents: \u001b[38;5;28mint\u001b[39m = \u001b[32m1\u001b[39m         \u001b[38;5;66;03m# Number of agents in the environment\u001b[39;00m\n\u001b[32m     13\u001b[39m     mass: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m1.0\u001b[39m           \u001b[38;5;66;03m# Mass of the drone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m params = \u001b[43mCrazyflieParams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_agents\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: EnvParams.__init__() got an unexpected keyword argument 'num_agents'"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class EnvParams:\n",
    "    \"\"\"\n",
    "    Base class for environment parameters.\n",
    "    Contains common attributes that all environment parameters should have.\n",
    "    \"\"\"\n",
    "\n",
    "class CrazyflieParams(EnvParams):\n",
    "    num_agents: int = 1         # Number of agents in the environment\n",
    "    mass: float = 1.0           # Mass of the drone\n",
    "\n",
    "params = CrazyflieParams(\n",
    "    num_agents=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2f09022e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1, 2],\n",
       "       [2, 3]], dtype=int32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.array([[1,2], [2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3d4d6233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-1.1055102 , -0.3212909 ,  0.3030113 ,  0.04504967],\n",
       "       [-0.25374556, -1.1386943 ,  0.69056857,  0.41803753],\n",
       "       [-0.4333626 ,  0.8505027 ,  0.68227184,  0.32156467],\n",
       "       [ 0.07891786, -1.190114  , -1.1471618 ,  0.5575204 ]],      dtype=float32)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = jax.random.uniform(jax.random.PRNGKey(10), (4, 4), minval=-1.2, maxval=1.2)\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "88fd6701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ True,  True, False,  True], dtype=bool)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.any(jnp.abs(pos) > jnp.array([1.0, 1.0, 1.0, 1.0]), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8cfdc578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "last_action = jax.random.uniform(key, (2, 10, 4), minval=-1, maxval=1)\n",
    "key = jax.random.PRNGKey(1)\n",
    "current_action = jax.random.uniform(key, (2, 10, 4), minval=-1, maxval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceefda6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_transpose() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[168]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminval\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: _transpose() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": [
    "jax.random.uniform(key, (2, 3, 3), minval=-1, maxval=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
